{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ce1a1e6029d44b2d8694f6627702d14c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_2b5fcd37149e47e2b2cfbcf8387558e6",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_c0bb5c7332b64cab8c7abee23c7d6351",
              "IPY_MODEL_2f3f227e1afc4fc98f84ed19ac4f9def"
            ]
          },
          "model_module_version": "1.5.0"
        },
        "2b5fcd37149e47e2b2cfbcf8387558e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          },
          "model_module_version": "1.2.0"
        },
        "c0bb5c7332b64cab8c7abee23c7d6351": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_cbc221967348483fb6c3d68634bc3b83",
            "_dom_classes": [],
            "description": " 21%",
            "_model_name": "FloatProgressModel",
            "bar_style": "danger",
            "max": 4926,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1034,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0e3d35ab7224434cb07323ab7236550a"
          },
          "model_module_version": "1.5.0"
        },
        "2f3f227e1afc4fc98f84ed19ac4f9def": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_8975042d7e1f44cfbe8c57d718c6d8a5",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1034/4926 [11:24&lt;42:55,  1.51it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3cc9b8c97be3472fa0a1e30a4d08f278"
          },
          "model_module_version": "1.5.0"
        },
        "cbc221967348483fb6c3d68634bc3b83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          },
          "model_module_version": "1.5.0"
        },
        "0e3d35ab7224434cb07323ab7236550a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          },
          "model_module_version": "1.2.0"
        },
        "8975042d7e1f44cfbe8c57d718c6d8a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          },
          "model_module_version": "1.5.0"
        },
        "3cc9b8c97be3472fa0a1e30a4d08f278": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          },
          "model_module_version": "1.2.0"
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5meH6GsMM4w",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "ce1a1e6029d44b2d8694f6627702d14c",
            "2b5fcd37149e47e2b2cfbcf8387558e6",
            "c0bb5c7332b64cab8c7abee23c7d6351",
            "2f3f227e1afc4fc98f84ed19ac4f9def",
            "cbc221967348483fb6c3d68634bc3b83",
            "0e3d35ab7224434cb07323ab7236550a",
            "8975042d7e1f44cfbe8c57d718c6d8a5",
            "3cc9b8c97be3472fa0a1e30a4d08f278"
          ]
        },
        "outputId": "9ff80cc8-ac36-4ff3-f992-bcd282845b9d"
      },
      "source": [
        "import sys\n",
        "import warnings\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import pyplot\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from math import sqrt\n",
        "import time;\n",
        "from sklearn import preprocessing\n",
        "import statsmodels.api as sm\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score\n",
        "from numpy import mean, absolute\n",
        "from sklearn.cluster import KMeans\n",
        "import random\n",
        "\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import tensorflow as tf\n",
        "\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers import Input, BatchNormalization, Activation, Dense, Dropout\n",
        "from keras.layers.core import Lambda, RepeatVector, Reshape\n",
        "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
        "from keras.layers.pooling import MaxPooling2D, GlobalMaxPool2D\n",
        "from keras.layers.merge import concatenate, add\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from keras.optimizers import Adam\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "\n",
        "\n",
        "\n",
        "######################################## Data Reading #####################################################################\n",
        "############################################################################################################\n",
        "\n",
        "\n",
        "print(\"------ Now Reading the data ------- Wait..\")\n",
        "print(\"   \")\n",
        "\n",
        "print(\"Normal usage: '28/2/2020 19:00'\")\n",
        "print(\"Normal usage: '14/2/2020 19:00'\")\n",
        "print(\"Normal usage: '11/2/2020 22:00'\")\n",
        "print(\"Normal usage: '2/2/2020 19:00'\")\n",
        "print(\"Sudden unusual consumption: '29/1/2020 22:00'\")\n",
        "print(\"Normal usage: '26/1/2020 23:00'\")\n",
        "print(\"Sudden unusual consumption: '13/1/2020 23:00'\")\n",
        "print(\"Possible illegal consumption: '5/1/2020 19:00'\")\n",
        "print(\"Normal usage: '19/12/2019 22:00'\")\n",
        "print(\"   \")\n",
        "\n",
        "\n",
        "\n",
        "Chaina_features =pd.read_csv('China1.csv', delimiter=',',encoding = 'unicode_escape') # select columns from 1 to 16 and store it\n",
        "#Chaina_features.info()\n",
        "#z1=Chaina_features.describe() # You can check the store data by describing them using the function\n",
        "#print(Chaina_features.shape)\n",
        "#Chaina_features = Chaina_features.set_index(Chaina_features.iloc[0:0,0:1036])\n",
        "print(\"------ Reading the data ------- Complete..\")\n",
        "\n",
        "#############################################################################################################\n",
        "############################################################################################################\n",
        "\n",
        "#############################################################################################################\n",
        "############################################################################################################\n",
        "print(\"------ fetching subset of data ------- wait..\")\n",
        "Chaina_features=Chaina_features.head(3000) #select first 2000 customers\n",
        "print(\"------ fetched subset of data ------- Complete..\")\n",
        "\n",
        "#############################################################################################################\n",
        "############################################################################################################\n",
        "#############################################################################################################\n",
        "############################################################################################################\n",
        "print(\"------ Imputation--Linear Interpolation method start------- wait..\")\n",
        "###https://www.geeksforgeeks.org/python-pandas-dataframe-interpolate/     ##description use to fil NA or NAN\n",
        "##limit : Maximum number of consecutive NaNs to fill. Must be greater than 0.\n",
        "Chaina_features = Chaina_features.interpolate(method='linear')\n",
        "Chaina_features=Chaina_features.replace(np.NaN, 0)\n",
        "print(\"------ Imputation--Linear Interpolation method ------- Complete..\")\n",
        "#############################################################################################################\n",
        "############################################################################################################\n",
        "#############################################################################################################\n",
        "############################################################################################################\n",
        "#############################################################################################################\n",
        "############################################################################################################\n",
        "\n",
        "features= Chaina_features.drop('CONS_NO', axis = 1)\n",
        "feature_list = list(features.columns)\n",
        "print(\"------ Outlier detection and removal 3 sigma method start------- wait..\")\n",
        "from numpy import std\n",
        "feature=np.array(features)\n",
        "data_mean, data_std = mean(feature), std(feature)\n",
        "# identify outliers\n",
        "cut_off = data_std * 3\n",
        "lower, upper = data_mean - cut_off, data_mean + cut_off\n",
        "# identify outliers\n",
        "outliers = [x for x in feature if (x < lower).any() or ( x > upper).any() ]\n",
        "print('Identified outliers: %d' % len(outliers))\n",
        "# remove outliers\n",
        "outliers_removed = [x for x in feature if (x >= lower).all() and (x <= upper).all()]\n",
        "print('Non-outlier observations: %d' % len(outliers_removed))\n",
        "\n",
        "newchaina=pd.DataFrame(outliers_removed, columns=feature_list)\n",
        "print(\"------ 3 sigma method ------- Complete..\")\n",
        "print(newchaina.shape)\n",
        "#############################################################################################################\n",
        "############################################################################################################\n",
        "#############################################################################################################\n",
        "############################################################################################################\n",
        "print(\"------ Proposed new sampling method start ------- wait..\")\n",
        "import new_sampling_technique\n",
        "sampl=new_sampling_technique.getsamples(newchaina)\n",
        "print(sampl.shape)\n",
        "print(\"------ Proposed new sampling method  ------- completed..\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#############################################################################################################\n",
        "############################################################################################################\n",
        "#############################################################################################################\n",
        "############################################################################################################\n",
        "#############################################################################################################\n",
        "############################################################################################################\n",
        "\n",
        "labels = np.array(sampl['FLAG'])#Target  feature\n",
        "features= sampl.drop('FLAG', axis = 1)\n",
        "feature_list = list(features.columns)#it store all features except  Reg_capacity price\n",
        "\n",
        "features_org=features;\n",
        "\n",
        "print('Data normalization---------------start')\n",
        "normalized_X=preprocessing.normalize(features)\n",
        "print('Data normalization---------------Complete')\n",
        "features = pd.DataFrame(normalized_X, columns=feature_list) # Convert to numpy array\n",
        "Copy_Features_Ext=features;\n",
        "print('------ Pre-processing ...Done..')\n",
        "#############################################################################################################\n",
        "\n",
        "print('------ Done..')\n",
        "print('------ Splitting the Data ------- Wait..')\n",
        "Feature_train, Feature_test, Label_train, Label_test = train_test_split(features, labels, test_size=0.25, random_state=7)\n",
        "print('Training Features Shape:', Feature_train.shape)\n",
        "print('Training Labels Shape:', Label_train.shape)\n",
        "print('Testing Features Shape:', Feature_test.shape)\n",
        "print('Testing Labels Shape:', Label_test.shape)\n",
        "print('------ Done..')\n",
        "print('################################')\n",
        "\n",
        "\n",
        "#############################################################################################################\n",
        "############################################################################################################\n",
        "#############################################################################################################\n",
        "############################################################################################################\n",
        "\n",
        "\n",
        "\n",
        "print(features.shape)\n",
        "print(labels.shape)\n",
        "\n",
        "'''\n",
        "results = model.fit(feat_train_lstm1, Label_train, batch_size=32, epochs=50, callbacks=callbacks,\\\n",
        "                    validation_data=(feat_train_lstm1, Label_train))\n",
        "'''\n",
        "\n",
        "# create features in 2-d LIKE 128 HEIGHT & 128 width\n",
        "X = np.zeros((len(features), 128, 128, 1), dtype=np.float32)\n",
        "y = np.zeros((len(labels), 128, 128, 1), dtype=np.float32)\n",
        "print(X.shape, y.shape)\n",
        "\n",
        "from tqdm import tqdm_notebook\n",
        "from skimage.transform import resize\n",
        "from skimage.morphology import label\n",
        "\n",
        "for n, id_ in tqdm_notebook(enumerate(features), total=len(features)):\n",
        "    # Load images\n",
        "    img = features\n",
        "    x_img = img_to_array(img)\n",
        "    x_img = resize(x_img, (128, 128, 1), mode = 'constant', preserve_range = True)\n",
        "    # Load masks\n",
        "    #mask = img_to_array(labels)\n",
        "    mask = resize(labels, (128, 128, 1), mode = 'constant', preserve_range = True)\n",
        "    # Save images after scaling in 128 + 128= 256\n",
        "    ## if before scaled then no need this\n",
        "    X[n] = x_img\n",
        "    y[n] = mask\n",
        "    print(\"making 2d data and adding -\"+str(n))\n",
        "\n",
        "print(X)\n",
        "\n",
        "\n",
        "# Split train and valid\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "#train=pd.DataFrame(X_train)\n",
        "#print(train)\n",
        "print('Training Features Shape:', X_train.shape)\n",
        "print('Training Labels Shape:', y_train.shape)\n",
        "print('Testing Features Shape:', X_valid.shape)\n",
        "print('Testing Labels Shape:', y_valid.shape)\n",
        "print('------ Done..')\n",
        "\n",
        "\n",
        "###########################Proposed Model start###################################\n",
        "#############################################################################################################\n",
        "############################################################################################################\n",
        "#############################################################################################################\n",
        "##################\n",
        "                        ###GRU\n",
        "##############################################################################################################\n",
        "##################################################################################################################\n",
        "\n",
        "from keras.layers import LeakyReLU\n",
        "from numpy import mean, absolute\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM, GRU\n",
        "from keras.layers import Dropout\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras import backend as K\n",
        "from keras import optimizers\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "\n",
        "feats_count = Feature_train.shape[1]\n",
        "samples_count = Feature_train.shape[0]\n",
        "print(samples_count)\n",
        "print(feats_count)\n",
        "Feature_train_lstm = np.expand_dims(Feature_train, axis=2)# new  shape  is  to be added\n",
        "Feature_test_lstm = np.expand_dims(Feature_test, axis=2)\n",
        "\n",
        "label_train_lstm = np.expand_dims(Label_train, axis=1)\n",
        "\n",
        "print(Feature_train_lstm.shape, Label_train.shape)\n",
        "print(Feature_train_lstm.shape, label_train_lstm.shape)\n",
        "\n",
        "def create_dnn():\n",
        "    K.clear_session()\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(GRU(units=30, return_sequences=True, input_shape=(1034, 1)))\n",
        "    model.add(LeakyReLU(alpha=0.001))\n",
        "    model.add(Dropout(0.1))\n",
        "    model.add(BatchNormalization())\n",
        "    '''Dropout layer is added to avoid over-fitting,\n",
        "    which is a phenomenon where a machine learning model performs better on the training data\n",
        "     compared to the test data. Execute the following script to add dropout layer.'''\n",
        "    #model.add(Dropout(0.2))\n",
        "    ##Let's add three more GRU and dropout layers to our model. Run the following script.\n",
        "\n",
        "\n",
        "    model.add(GRU(units=10))\n",
        "    model.add(LeakyReLU(alpha=0.001))\n",
        "    model.add(Dropout(0.1))\n",
        "\n",
        "    model.add(Dense(units=1))\n",
        "\n",
        "    '''To make our model more robust, we add a dense layer at the end of the model.\n",
        "     The number of neurons in the dense layer will be set to 1 since we want to predict a single value in the output.\n",
        "    '''\n",
        "    model.summary()\n",
        "    model.compile(optimizer = \"adam\", loss = \"binary_crossentropy\")\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "dnn_model = KerasClassifier(build_fn=create_dnn, batch_size=32, verbose=1)\n",
        "\n",
        "callbacks = [\n",
        "    EarlyStopping(patience=10, verbose=1),\n",
        "    ReduceLROnPlateau(factor=0.1, patience=5, min_lr=0.00001, verbose=1)]\n",
        "\n",
        "history=dnn_model.fit(Feature_train_lstm, Label_train, batch_size = 32, epochs=15\n",
        "              ,validation_data=(Feature_test_lstm, Label_test), callbacks=callbacks)\n",
        "y_pred1=dnn_model.predict(Feature_test_lstm)\n",
        "\n",
        "y_pred1=dnn_model.predict(Feature_train_lstm)\n",
        "y_pred2=dnn_model.predict(Feature_test_lstm)\n",
        "print(y_pred2)\n",
        "print(y_pred1)\n",
        "\n",
        "\n",
        "\n",
        "plt.figure(figsize=(6,8))\n",
        "plt.title(\"Learning curve\")\n",
        "plt.plot(history.history[\"loss\"],'-b', label=\"loss\")\n",
        "plt.plot(history.history[\"val_loss\"],'-r', label=\"val_loss\")\n",
        "plt.plot( np.argmin(history.history[\"val_loss\"]), np.min(history.history[\"val_loss\"]), marker=\"x\", color=\"black\", label=\"best model\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"log_loss\")\n",
        "plt.legend();\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "############################################################################################################\n",
        "#####################################################DenseNetFCN modulel########################################################\n",
        "############################################################################################################\n",
        "\n",
        "def conv2d_block(input_tensor, n_filters, kernel_size = 3, batchnorm = True):\n",
        "    \"\"\"Function to add 2 convolutional layers with the parameters passed to it\"\"\"\n",
        "    # first layer\n",
        "    x = Conv2D(filters = n_filters, kernel_size = (kernel_size, kernel_size),\\\n",
        "              kernel_initializer = 'he_normal', padding = 'same')(input_tensor)\n",
        "    if batchnorm:\n",
        "        x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    # second layer\n",
        "    x = Conv2D(filters = n_filters, kernel_size = (kernel_size, kernel_size),\\\n",
        "              kernel_initializer = 'he_normal', padding = 'same')(input_tensor)\n",
        "    if batchnorm:\n",
        "        x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "def get_denseNetFCN(input_img, n_filters = 64, dropout = 0.1, batchnorm = True):\n",
        "    \"\"\"Function to define the DenseFCN Model\"\"\"\n",
        "\n",
        "    c1 = conv2d_block(input_img, n_filters * 1, kernel_size = 3, batchnorm = batchnorm)\n",
        "    p1 = MaxPooling2D((2, 2))(c1)\n",
        "    p1 = Dropout(dropout)(p1)\n",
        "\n",
        "    c2 = conv2d_block(p1, n_filters * 2, kernel_size = 3, batchnorm = batchnorm)\n",
        "    p2 = MaxPooling2D((2, 2))(c2)\n",
        "    p2 = Dropout(dropout)(p2)\n",
        "\n",
        "    c3 = conv2d_block(p2, n_filters * 4, kernel_size = 3, batchnorm = batchnorm)\n",
        "    p3 = MaxPooling2D((2, 2))(c3)\n",
        "    p3 = Dropout(dropout)(p3)\n",
        "\n",
        "    c4 = conv2d_block(p3, n_filters * 8, kernel_size = 3, batchnorm = batchnorm)\n",
        "    p4 = MaxPooling2D((2, 2))(c4)\n",
        "    p4 = Dropout(dropout)(p4)\n",
        "\n",
        "    c5 = conv2d_block(p4, n_filters = n_filters * 16, kernel_size = 3, batchnorm = batchnorm)\n",
        "\n",
        "\n",
        "    u6 = Conv2DTranspose(n_filters * 8, (3, 3), strides = (2, 2), padding = 'same')(c5)\n",
        "    u6 = concatenate([u6, c4])\n",
        "    u6 = Dropout(dropout)(u6)\n",
        "    c6 = conv2d_block(u6, n_filters * 8, kernel_size = 3, batchnorm = batchnorm)\n",
        "\n",
        "    u7 = Conv2DTranspose(n_filters * 4, (3, 3), strides = (2, 2), padding = 'same')(c6)\n",
        "    u7 = concatenate([u7, c3])\n",
        "    u7 = Dropout(dropout)(u7)\n",
        "    c7 = conv2d_block(u7, n_filters * 4, kernel_size = 3, batchnorm = batchnorm)\n",
        "\n",
        "    u8 = Conv2DTranspose(n_filters * 2, (3, 3), strides = (2, 2), padding = 'same')(c7)\n",
        "    u8 = concatenate([u8, c2])\n",
        "    u8 = Dropout(dropout)(u8)\n",
        "    c8 = conv2d_block(u8, n_filters * 2, kernel_size = 3, batchnorm = batchnorm)\n",
        "\n",
        "    u9 = Conv2DTranspose(n_filters * 1, (3, 3), strides = (2, 2), padding = 'same')(c8)\n",
        "    u9 = concatenate([u9, c1])\n",
        "    u9 = Dropout(dropout)(u9)\n",
        "    c9 = conv2d_block(u9, n_filters * 1, kernel_size = 3, batchnorm = batchnorm)\n",
        "\n",
        "    outputs = Conv2D(1, (1, 1), activation='sigmoid')(c9)\n",
        "    model = Model(inputs=[input_img], outputs=[outputs], name=\"DenseNetFCN\")\n",
        "    return model\n",
        "\n",
        "input_img = Input((128, 128, 1))\n",
        "model = get_denseNetFCN(input_img, n_filters=16, dropout=0.01, batchnorm=True)\n",
        "model.compile(optimizer=Adam(), loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "model.summary()\n",
        "\n",
        "\n",
        "callbacks = [\n",
        "    EarlyStopping(patience=10, verbose=1),\n",
        "    ReduceLROnPlateau(factor=0.1, patience=5, min_lr=0.00001, verbose=1),\n",
        "    ModelCheckpoint('model-tgs-salt.h5', verbose=1,save_best_only=True, save_weights_only=True)\n",
        "]\n",
        "\n",
        "\n",
        "results = model.fit(X_train, y_train, batch_size=32, epochs=15, callbacks=callbacks,\\\n",
        "                    validation_data=(X_valid, y_valid))\n",
        "\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(results.history[\"loss\"], '-b', label=\"loss\")\n",
        "plt.plot(results.history[\"val_loss\"], '-g', label=\"val_loss\")\n",
        "plt.plot( np.argmin(results.history[\"val_loss\"]), np.min(results.history[\"val_loss\"]), marker=\"x\", color=\"black\", label=\"best model\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"log_loss\")\n",
        "plt.legend();\n",
        "\n",
        "model.load_weights('model-tgs-salt.h5')\n",
        "\n",
        "# Evaluate on validation set (this must be equals to the best log_loss)\n",
        "##tells validation loss and accuracy\n",
        "model.evaluate(X_valid, y_valid, verbose=1)\n",
        "\n",
        "preds_train = model.predict(X_train, verbose=1)\n",
        "preds_val = model.predict(X_valid, verbose=1)\n",
        "densentfcntrain=Feature_train\n",
        "densentfcntest=Feature_test\n",
        "print(preds_train.shape, preds_val.shape)\n",
        "\n",
        "\n",
        "\n",
        "import lightgbm as lgb\n",
        "\n",
        "joinedtrain=np.concatenate((densentfcntrain,y_pred1), axis=1)\n",
        "joinedtest=np.concatenate((densentfcntest,y_pred2), axis=1)\n",
        "\n",
        "classifier = lgb.LGBMClassifier(max_depth = 5,\n",
        "                         lambda_l1 = 0.1,\n",
        "                         lambda_l2 = 0.01,\n",
        "                         learning_rate = 0.01,\n",
        "                         n_estimators = 200, reg_alpha = 1.1, colsample_bytree = 0.9, subsample = 0.9,\n",
        "                         n_jobs = 5)\n",
        "classifier.fit(joinedtrain, Label_train, eval_set=[(joinedtest, Label_test)], eval_metric='accuracy',\n",
        "               verbose=False, early_stopping_rounds=50);\n",
        "classifier.fit(joinedtest, Label_test, eval_set=[(joinedtrain, Label_train)], eval_metric='accuracy',\n",
        "               verbose=False, early_stopping_rounds=50);\n",
        "\n",
        "predictions = classifier.predict(joinedtest)\n",
        "#predictions = classifier.predict(joinedtest)\n",
        "a = accuracy_score(Label_test, classifier.predict(joinedtest))\n",
        "b = accuracy_score(Label_train, classifier.predict(joinedtrain))\n",
        "\n",
        "print(\"Accuracy on test data\",a,\"Accuracy on train data\",b)\n",
        "\n",
        "#############################################################################################################\n",
        "################################################Validation############################################################\n",
        "#############################################################################################################\n",
        "############################################################################################################\n",
        "\n",
        "print(\"Confusion matrix is: \", confusion_matrix(Label_test, predictions))\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "f1score=f1_score(Label_test, predictions, average='binary')\n",
        "print(\"F1score is: \", f1score)\n",
        "\n",
        "\n",
        "from sklearn.metrics import recall_score\n",
        "recall = recall_score(Label_test, predictions)\n",
        "print('Recall score: %f' % recall)\n",
        "\n",
        "from sklearn.metrics import precision_score\n",
        "precision = precision_score(Label_test, predictions)\n",
        "print('Precision: %f' % precision)\n",
        "\n",
        "\n",
        "# ROC AUC score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "auc = roc_auc_score(Label_test, predictions)\n",
        "print('ROC AUC: %f' % auc)\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import plot_precision_recall_curve\n",
        "\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "MCC=matthews_corrcoef(Label_test, predictions)\n",
        "print('MCC: %f' % MCC)\n",
        "\n",
        "\n",
        "\n",
        "# ROC AUC plot\n",
        "from sklearn.metrics import roc_curve\n",
        "fpr1_lr, tpr1_lr, _= roc_curve(Label_test, predictions)\n",
        "plt.figure(1)\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.plot(fpr1_lr, tpr1_lr, label='Proposed model')\n",
        "plt.xlabel('False positive rate')\n",
        "plt.ylabel('True positive rate')\n",
        "plt.title('ROC curve')\n",
        "plt.legend(loc='best')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.metrics import average_precision_score\n",
        "average_precision = average_precision_score(Label_test, predictions)\n",
        "print('Average precision-recall score: {0:0.2f}'.format(\n",
        "      average_precision))\n",
        "\n",
        "\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import plot_precision_recall_curve\n",
        "import matplotlib.pyplot as plt\n",
        "disp = plot_precision_recall_curve(classifier, joinedtest, Label_test, name=\"Our proposed model\")\n",
        "disp.ax_.set_title('2-class Precision-Recall curve: '\n",
        "                   'AP={0:0.2f}'.format(average_precision))\n",
        "\n",
        "\n",
        "############################################Proposed model end ########################################\n",
        "\n",
        "################################## model 1 Wide n Deep CNN   ###################\n",
        "#############################################################################################################\n",
        "############################################################################################################\n",
        "#############################################################################################################\n",
        "############################################################################################################\n",
        "                                        ### CNN\n",
        "print(\"W&D-CNN ----------------start\")\n",
        "###################################################################################\n",
        "from keras.models import Sequential      # Model\n",
        "from keras.layers import Dense            # lAYER\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import MaxPooling1D\n",
        "from keras.layers import Conv1D\n",
        "from keras.layers import Convolution2D\n",
        "from keras.layers import Flatten\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras import backend as K\n",
        "\n",
        "\n",
        "feats_count = Feature_train.shape[1]\n",
        "samples_count = Feature_test.shape[0]\n",
        "print(samples_count)\n",
        "print(feats_count)\n",
        "Feature_train_CNN = np.expand_dims(Feature_train, axis=2)# new  shape  is  to be added\n",
        "Feature_test_CNN = np.expand_dims(Feature_test, axis=2)\n",
        "\n",
        "\n",
        "K.clear_session()\n",
        "wide = Sequential()\n",
        "\n",
        "wide.add(Conv1D(32, 3, input_shape=(1034,1)))\n",
        "wide.add(Activation('relu'))\n",
        "wide.add(MaxPooling1D(pool_size=(2)))\n",
        "wide.add(Flatten()) # this converts our 3D feature maps to 1D feature vectors\n",
        "wide.add(Dense(1))\n",
        "wide.add(Activation('sigmoid'))\n",
        "wide.compile(optimizer = 'adam', loss = 'binary_crossentropy',metrics=['accuracy'])\n",
        "wide.summary()\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=10,min_delta=1)\n",
        "wide.fit(Feature_train_CNN, Label_train, epochs=15, batch_size=32, verbose=1,validation_split=0.25,callbacks=[early_stop])\n",
        "wide_pred=wide.predict(Feature_train_CNN);\n",
        "wide_pred1=wide.predict(Feature_test_CNN);\n",
        "\n",
        "\n",
        "batch_size = 32\n",
        "nb_filters1 = 32\n",
        "nb_filters2 = 64\n",
        "conv1_size = 3\n",
        "conv2_size = 2\n",
        "pool_size = 2\n",
        "classes_num = 1\n",
        "\n",
        "K.clear_session()\n",
        "cnn = Sequential()\n",
        "\n",
        "cnn.add(Conv2D(32, (3, 3), input_shape=(128,128,1)))\n",
        "cnn.add(Activation('relu'))\n",
        "cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "cnn.add(Conv2D(32, (3, 3)))\n",
        "cnn.add(Activation('relu'))\n",
        "cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "cnn.add(Conv2D(64, (3, 3)))\n",
        "cnn.add(Activation('relu'))\n",
        "cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# the model so far outputs 3D feature maps (height, width, features)\n",
        "\n",
        "cnn.add(Flatten()) # this converts our 3D feature maps to 1D feature vectors\n",
        "\n",
        "cnn.add(Dense(1))\n",
        "cnn.add(Activation('sigmoid'))\n",
        "\n",
        "cnn.compile(optimizer = 'adam', loss = 'binary_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "cnn.summary()\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=10,min_delta=1)\n",
        "cnn.fit(X_train, Label_train, epochs=10, batch_size=32, verbose=1,validation_split=0.25,callbacks=[early_stop])\n",
        "cnn_pred=cnn.predict(X_valid);  # Used to predict\n",
        "\n",
        "#############################################################################################################\n",
        "wide_pred1=wide_pred1.round()\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "f1score=f1_score(Label_test, wide_pred1, average='binary')\n",
        "print(\"F1score is: \", f1score)\n",
        "\n",
        "\n",
        "print(\"Accuracy:\",metrics.accuracy_score(Label_test,wide_pred1))\n",
        "\n",
        "print(\"Precision:\",metrics.precision_score(Label_test, wide_pred1))\n",
        "\n",
        "print(\"Recall:\",metrics.recall_score(Label_test, wide_pred1))\n",
        "\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import plot_precision_recall_curve\n",
        "from sklearn.metrics import average_precision_score\n",
        "average_precision = average_precision_score(Label_test, wide_pred1)\n",
        "print('Average precision-recall score: {0:0.2f}'.format(\n",
        "      average_precision))\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "MCC=matthews_corrcoef(Label_test, wide_pred1)\n",
        "print('MCC: %f' % MCC)\n",
        "\n",
        "\n",
        "# ROC AUC\n",
        "from sklearn.metrics import roc_auc_score\n",
        "auc = roc_auc_score(Label_test, wide_pred1)\n",
        "print('ROC AUC: %f' % auc)\n",
        "\n",
        "fpr2_lr, tpr2_lr, _= roc_curve(Label_test, wide_pred1)\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.plot(fpr2_lr, tpr2_lr, label='W&D-CNN')\n",
        "plt.xlabel('False positive rate')\n",
        "plt.ylabel('True positive rate')\n",
        "plt.title('ROC curve')\n",
        "plt.legend(loc='best')\n",
        "plt.show()\n",
        "\n",
        "print(\"W&D-CNN ----------------end\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "################################ Model 2 CNN-LSTM-MLP start #####################################\n",
        "\n",
        "################################ Model 2 CNN-LSTM-MLP start #####################################\n",
        "#                        ###Lstm\n",
        "###############################################################################################################\n",
        "###################################################################################################################\n",
        "#\n",
        "from keras.layers import LeakyReLU\n",
        "from numpy import mean, absolute\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM, GRU\n",
        "from keras.layers import Dropout\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras import backend as K\n",
        "from keras import optimizers\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "\n",
        "feats_count = Feature_train.shape[1]\n",
        "samples_count = Feature_train.shape[0]\n",
        "print(samples_count)\n",
        "print(feats_count)\n",
        "Feature_train_lstm = np.expand_dims(Feature_train, axis=2)# new  shape  is  to be added\n",
        "Feature_test_lstm = np.expand_dims(Feature_test, axis=2)\n",
        "\n",
        "label_train_lstm = np.expand_dims(Label_train, axis=1)\n",
        "\n",
        "print(Feature_train_lstm.shape, Label_train.shape)\n",
        "print(Feature_train_lstm.shape, label_train_lstm.shape)\n",
        "\n",
        "def create_dnn():\n",
        "    K.clear_session()\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(LSTM(units=20, return_sequences=True, input_shape=(1034, 1)))\n",
        "    model.add(LeakyReLU(alpha=0.001))\n",
        "    model.add(Dropout(0.1))\n",
        "    model.add(BatchNormalization())\n",
        "    '''Dropout layer is added to avoid over-fitting,\n",
        "    which is a phenomenon where a machine learning model performs better on the training data\n",
        "     compared to the test data. Execute the following script to add dropout layer.'''\n",
        "    #model.add(Dropout(0.2))\n",
        "    ##Let's add three more GRU and dropout layers to our model. Run the following script.\n",
        "\n",
        "\n",
        "    model.add(LSTM(units=20))\n",
        "    model.add(LeakyReLU(alpha=0.001))\n",
        "    model.add(Dropout(0.1))\n",
        "\n",
        "    model.add(Dense(units=1))\n",
        "\n",
        "    '''To make our model more robust, we add a dense layer at the end of the model.\n",
        "     The number of neurons in the dense layer will be set to 1 since we want to predict a single value in the output.\n",
        "    '''\n",
        "    model.summary()\n",
        "    model.compile(optimizer = \"adam\", loss = \"binary_crossentropy\")\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "dnn_model = KerasClassifier(build_fn=create_dnn, batch_size=32, verbose=1)\n",
        "\n",
        "callbacks = [\n",
        "    EarlyStopping(patience=10, verbose=1),\n",
        "    ReduceLROnPlateau(factor=0.1, patience=5, min_lr=0.00001, verbose=1)]\n",
        "\n",
        "history=dnn_model.fit(Feature_train_lstm, Label_train, batch_size = 32, epochs=15\n",
        "              ,validation_data=(Feature_test_lstm, Label_test), callbacks=callbacks)\n",
        "y_pred1=dnn_model.predict(Feature_test_lstm)\n",
        "\n",
        "y_pred1=dnn_model.predict(Feature_train_lstm)\n",
        "y_pred2=dnn_model.predict(Feature_test_lstm)\n",
        "print(y_pred2)\n",
        "print(y_pred1)\n",
        "\n",
        "\n",
        "\n",
        "plt.figure(figsize=(6,8))\n",
        "plt.title(\"Learning curve\")\n",
        "plt.plot(history.history[\"loss\"],'-b', label=\"loss\")\n",
        "plt.plot(history.history[\"val_loss\"],'-r', label=\"val_loss\")\n",
        "plt.plot( np.argmin(history.history[\"val_loss\"]), np.min(history.history[\"val_loss\"]), marker=\"x\", color=\"black\", label=\"best model\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"log_loss\")\n",
        "plt.legend();\n",
        "\n",
        "\n",
        "#######################################CNN#############################################\n",
        "#########################################################################################################\n",
        "\n",
        "\n",
        "\n",
        "from keras.models import Sequential      # Model\n",
        "from keras.layers import Dense            # lAYER\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import MaxPooling1D\n",
        "from keras.layers import Conv1D\n",
        "from keras.layers import Flatten\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras import backend as K\n",
        "\n",
        "\n",
        "feats_count = Feature_train.shape[1]\n",
        "samples_count = Feature_train.shape[0]\n",
        "print(samples_count)\n",
        "print(feats_count)\n",
        "Feature_train_CNN = np.expand_dims(Feature_train, axis=2)# new  shape  is  to be added\n",
        "Feature_test_CNN = np.expand_dims(Feature_test, axis=2)\n",
        "\n",
        "label_train_CNN = np.expand_dims(Label_train, axis=1)\n",
        "\n",
        "print(Feature_train_CNN.shape, Label_train.shape)\n",
        "print(Feature_train_CNN.shape, label_train_CNN.shape)\n",
        "K.clear_session()\n",
        "classifier = Sequential()\n",
        "classifier.add(Conv1D(32, kernel_size=(3), activation='relu', padding='same' ,input_shape=(feats_count,1)))\n",
        "print('FT:',(len(np.transpose(Feature_train)))); # Units are the neurons in layer\n",
        "classifier.add(MaxPooling1D(pool_size=2,padding='same'))\n",
        "classifier.add(Dropout(0.0001));\n",
        "classifier.add(Flatten())\n",
        "classifier.add(Dense(2))\n",
        "classifier.add(Dropout(0.01)); #Dropout is a a technique used to tackle Overfitting and reducre complexity\n",
        "    #Dropout is a regularization technique, which aims to reduce the complexity of the model with the goal to prevent overfitting.\n",
        "classifier.add(Dense(1)) # A dense Layer have 1 neuron. it works good in my Scenerio. you can change it\n",
        "classifier.compile(optimizer = 'adam', loss = \"binary_crossentropy\" )\n",
        "    #Compile:Before training a model, you need to configure the learning process, which is done via the compile\n",
        "    ##loss function: This is the objective that the model will try to minimize. It can be the string identifier of an existing loss function (such as categorical_crossentropy or mse), or it can be an objective function\n",
        "    ##metrics: For any classification problem you will want to set this to metrics=['accuracy']. A metric could be the string identifier of an existing metric or a custom metric function.\n",
        "classifier.summary()\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=10,min_delta=1)\n",
        "history = classifier.fit(Feature_train_CNN, Label_train, epochs=100, batch_size=25, verbose=0,validation_split=0.25,callbacks=[early_stop])\n",
        "predictions_CNN=classifier.predict(Feature_test_CNN);  # Used to predict\n",
        "predictions_CNN=predictions_CNN;\n",
        "print(predictions_CNN.shape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "plt.figure(figsize=(6,8))\n",
        "plt.title(\"Learning curve\")\n",
        "plt.plot(history.history[\"loss\"],'-b', label=\"loss\")\n",
        "plt.plot(history.history[\"val_loss\"],'-r', label=\"val_loss\")\n",
        "plt.plot( np.argmin(history.history[\"val_loss\"]), np.min(history.history[\"val_loss\"]), marker=\"x\", color=\"black\", label=\"best model\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"log_loss\")\n",
        "plt.legend();\n",
        "\n",
        "\n",
        "\n",
        "####################################### MLP #########################################\n",
        "\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "joinedtrain=np.concatenate((Feature_train,y_pred1), axis=1)\n",
        "joinedtest=np.concatenate((Feature_test,y_pred2), axis=1)\n",
        "\n",
        "print('MLP Classifier start from here:')\n",
        "#########################################################################################\n",
        "classifier=MLPClassifier(solver='lbfgs',alpha=0.0001, verbose=0, max_iter=200)\n",
        "classifier.fit(joinedtrain, Label_train);\n",
        "lstmcnnmlp=classifier.predict(joinedtest)\n",
        "print('MLP Classifier end here:')\n",
        "\n",
        "\n",
        "print(\"Confusion matrix is: \", confusion_matrix(Label_test, lstmcnnmlp))\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "f1score=f1_score(Label_test, lstmcnnmlp, average='binary')\n",
        "print(\"F1score is: \", f1score)\n",
        "\n",
        "\n",
        "from sklearn.metrics import recall_score\n",
        "recall = recall_score(Label_test, lstmcnnmlp)\n",
        "print('Recall score: %f' % recall)\n",
        "\n",
        "from sklearn.metrics import precision_score\n",
        "precision = precision_score(Label_test, lstmcnnmlp)\n",
        "print('Precision: %f' % precision)\n",
        "\n",
        "\n",
        "# ROC AUC score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "auc = roc_auc_score(Label_test, lstmcnnmlp)\n",
        "print('ROC AUC: %f' % auc)\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import plot_precision_recall_curve\n",
        "\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "MCC=matthews_corrcoef(Label_test, lstmcnnmlp)\n",
        "print('MCC: %f' % MCC)\n",
        "\n",
        "\n",
        "\n",
        "# ROC AUC plot\n",
        "from sklearn.metrics import roc_curve\n",
        "fpr3_lr, tpr3_lr, _ = roc_curve(Label_test, lstmcnnmlp)\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.plot(fpr3_lr, tpr3_lr, label='Proposed model')\n",
        "plt.xlabel('False positive rate')\n",
        "plt.ylabel('True positive rate')\n",
        "plt.title('ROC curve')\n",
        "plt.legend(loc='best')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.metrics import average_precision_score\n",
        "\n",
        "## plot the precision-recall curves\n",
        "#no_skill = len(Label_test[[predictions]==100]) / len(predictions)\n",
        "#pyplot.plot([0, 1], [0, 1],[no_skill, no_skill], linestyle='--', label='No Skill')\n",
        "#pyplot.plot(recall, precision, marker='.', label='Proposed')\n",
        "## axis labels\n",
        "#pyplot.xlabel('Recall')\n",
        "#pyplot.ylabel('Precision')\n",
        "## show the legend\n",
        "#pyplot.legend()\n",
        "## show the plot\n",
        "#pyplot.show()\n",
        "\n",
        "\n",
        "average_precision = average_precision_score(Label_test, lstmcnnmlp)\n",
        "print('Average precision-recall score: {0:0.2f}'.format(\n",
        "      average_precision))\n",
        "\n",
        "\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import plot_precision_recall_curve\n",
        "import matplotlib.pyplot as plt\n",
        "disp = plot_precision_recall_curve(classifier, joinedtest, Label_test, name=\"Our proposed model\")\n",
        "disp.ax_.set_title('2-class Precision-Recall curve: '\n",
        "                   'AP={0:0.2f}'.format(average_precision))\n",
        "\n",
        "\n",
        "\n",
        "################################ Model 2 CNN-LSTM-MLP end #####################################\n",
        "\n",
        "############################# Model 3 GRU-MLP start ##############################\n",
        "\n",
        "\n",
        "                        ###GRU\n",
        "##############################################################################################################\n",
        "##################################################################################################################\n",
        "\n",
        "from keras.layers import LeakyReLU\n",
        "from numpy import mean, absolute\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM, GRU\n",
        "from keras.layers import Dropout\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras import backend as K\n",
        "from keras import optimizers\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "\n",
        "feats_count = Feature_train.shape[1]\n",
        "samples_count = Feature_train.shape[0]\n",
        "print(samples_count)\n",
        "print(feats_count)\n",
        "Feature_train_lstm = np.expand_dims(Feature_train, axis=2)# new  shape  is  to be added\n",
        "Feature_test_lstm = np.expand_dims(Feature_test, axis=2)\n",
        "\n",
        "label_train_lstm = np.expand_dims(Label_train, axis=1)\n",
        "\n",
        "print(Feature_train_lstm.shape, Label_train.shape)\n",
        "print(Feature_train_lstm.shape, label_train_lstm.shape)\n",
        "\n",
        "def create_dnn():\n",
        "    K.clear_session()\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(GRU(units=30, return_sequences=True, input_shape=(1034, 1)))\n",
        "    model.add(LeakyReLU(alpha=0.001))\n",
        "    model.add(Dropout(0.1))\n",
        "    model.add(BatchNormalization())\n",
        "    '''Dropout layer is added to avoid over-fitting,\n",
        "    which is a phenomenon where a machine learning model performs better on the training data\n",
        "     compared to the test data. Execute the following script to add dropout layer.'''\n",
        "    #model.add(Dropout(0.2))\n",
        "    ##Let's add three more GRU and dropout layers to our model. Run the following script.\n",
        "\n",
        "\n",
        "    model.add(GRU(units=10))\n",
        "    model.add(LeakyReLU(alpha=0.001))\n",
        "    model.add(Dropout(0.1))\n",
        "\n",
        "    model.add(Dense(units=1))\n",
        "\n",
        "    '''To make our model more robust, we add a dense layer at the end of the model.\n",
        "     The number of neurons in the dense layer will be set to 1 since we want to predict a single value in the output.\n",
        "    '''\n",
        "    model.summary()\n",
        "    model.compile(optimizer = \"adam\", loss = \"binary_crossentropy\")\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "dnn_model = KerasClassifier(build_fn=create_dnn, batch_size=32, verbose=1)\n",
        "\n",
        "callbacks = [\n",
        "    EarlyStopping(patience=10, verbose=1),\n",
        "    ReduceLROnPlateau(factor=0.1, patience=5, min_lr=0.00001, verbose=1)]\n",
        "\n",
        "history=dnn_model.fit(Feature_train_lstm, Label_train, batch_size = 32, epochs=15\n",
        "              ,validation_data=(Feature_test_lstm, Label_test), callbacks=callbacks)\n",
        "y_pred1=dnn_model.predict(Feature_test_lstm)\n",
        "\n",
        "y_pred1=dnn_model.predict(Feature_train_lstm)\n",
        "y_pred2=dnn_model.predict(Feature_test_lstm)\n",
        "print(y_pred2)\n",
        "print(y_pred1)\n",
        "\n",
        "\n",
        "\n",
        "plt.figure(figsize=(6,8))\n",
        "plt.title(\"Learning curve\")\n",
        "plt.plot(history.history[\"loss\"],'-b', label=\"loss\")\n",
        "plt.plot(history.history[\"val_loss\"],'-r', label=\"val_loss\")\n",
        "plt.plot( np.argmin(history.history[\"val_loss\"]), np.min(history.history[\"val_loss\"]), marker=\"x\", color=\"black\", label=\"best model\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"log_loss\")\n",
        "plt.legend();\n",
        "\n",
        "####################################### MLP #########################################\n",
        "\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "joinedtrain=np.concatenate((Feature_train,y_pred1), axis=1)\n",
        "joinedtest=np.concatenate((Feature_test,y_pred2), axis=1)\n",
        "\n",
        "print('MLP Classifier start from here:')\n",
        "#########################################################################################\n",
        "classifier=MLPClassifier(solver='lbfgs',alpha=0.0001, verbose=0, max_iter=200)\n",
        "classifier.fit(joinedtrain, Label_train);\n",
        "grumlp=classifier.predict(joinedtest)\n",
        "print('MLP Classifier end here:')\n",
        "\n",
        "\n",
        "print(\"Confusion matrix is: \", confusion_matrix(Label_test, grumlp))\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "f1score=f1_score(Label_test, grumlp, average='binary')\n",
        "print(\"F1score is: \", f1score)\n",
        "\n",
        "\n",
        "from sklearn.metrics import recall_score\n",
        "recall = recall_score(Label_test, grumlp)\n",
        "print('Recall score: %f' % recall)\n",
        "\n",
        "from sklearn.metrics import precision_score\n",
        "precision = precision_score(Label_test, grumlp)\n",
        "print('Precision: %f' % precision)\n",
        "\n",
        "\n",
        "# ROC AUC score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "auc = roc_auc_score(Label_test, grumlp)\n",
        "print('ROC AUC: %f' % auc)\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import plot_precision_recall_curve\n",
        "\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "MCC=matthews_corrcoef(Label_test, grumlp)\n",
        "print('MCC: %f' % MCC)\n",
        "\n",
        "\n",
        "\n",
        "# ROC AUC plot\n",
        "from sklearn.metrics import roc_curve\n",
        "fpr4_lr, tpr4_lr, _= roc_curve(Label_test, grumlp)\n",
        "plt.figure(1)\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.plot(fpr4_lr, tpr4_lr, label='Proposed model')\n",
        "plt.xlabel('False positive rate')\n",
        "plt.ylabel('True positive rate')\n",
        "plt.title('ROC curve')\n",
        "plt.legend(loc='best')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# plot the precision-recall curves\n",
        "no_skill = len(Label_test[Label_test==1]) / len(Label_test)\n",
        "pyplot.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill')\n",
        "pyplot.plot(recall, precision, marker='.', label='Proposed')\n",
        "# axis labels\n",
        "pyplot.xlabel('Recall')\n",
        "pyplot.ylabel('Precision')\n",
        "# show the legend\n",
        "pyplot.legend()\n",
        "# show the plot\n",
        "pyplot.show()\n",
        "\n",
        "\n",
        "from sklearn.metrics import average_precision_score\n",
        "average_precision = average_precision_score(Label_test, grumlp)\n",
        "print('Average precision-recall score: {0:0.2f}'.format(\n",
        "      average_precision))\n",
        "\n",
        "\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import plot_precision_recall_curve\n",
        "import matplotlib.pyplot as plt\n",
        "disp = plot_precision_recall_curve(classifier, joinedtest, Label_test, name=\"Our proposed model\")\n",
        "disp.ax_.set_title('2-class Precision-Recall curve: '\n",
        "                   'AP={0:0.2f}'.format(average_precision))\n",
        "\n",
        "\n",
        "############################# Model 3 GRU-MLP end ##############################\n",
        "\n",
        "\n",
        "############################# Model 4 LSTM-MLP start ##############################\n",
        "\n",
        "\n",
        "\n",
        "#                        ###Lstm\n",
        "###############################################################################################################\n",
        "###################################################################################################################\n",
        "#\n",
        "from keras.layers import LeakyReLU\n",
        "from numpy import mean, absolute\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM, GRU\n",
        "from keras.layers import Dropout\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras import backend as K\n",
        "from keras import optimizers\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "\n",
        "feats_count = Feature_train.shape[1]\n",
        "samples_count = Feature_train.shape[0]\n",
        "print(samples_count)\n",
        "print(feats_count)\n",
        "Feature_train_lstm = np.expand_dims(Feature_train, axis=2)# new  shape  is  to be added\n",
        "Feature_test_lstm = np.expand_dims(Feature_test, axis=2)\n",
        "\n",
        "label_train_lstm = np.expand_dims(Label_train, axis=1)\n",
        "\n",
        "print(Feature_train_lstm.shape, Label_train.shape)\n",
        "print(Feature_train_lstm.shape, label_train_lstm.shape)\n",
        "\n",
        "def create_dnn():\n",
        "    K.clear_session()\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(LSTM(units=20, return_sequences=True, input_shape=(1034, 1)))\n",
        "    model.add(LeakyReLU(alpha=0.001))\n",
        "    model.add(Dropout(0.1))\n",
        "    model.add(BatchNormalization())\n",
        "    '''Dropout layer is added to avoid over-fitting,\n",
        "    which is a phenomenon where a machine learning model performs better on the training data\n",
        "     compared to the test data. Execute the following script to add dropout layer.'''\n",
        "    #model.add(Dropout(0.2))\n",
        "    ##Let's add three more GRU and dropout layers to our model. Run the following script.\n",
        "\n",
        "\n",
        "    model.add(LSTM(units=20))\n",
        "    model.add(LeakyReLU(alpha=0.001))\n",
        "    model.add(Dropout(0.1))\n",
        "\n",
        "    model.add(Dense(units=1))\n",
        "\n",
        "    '''To make our model more robust, we add a dense layer at the end of the model.\n",
        "     The number of neurons in the dense layer will be set to 1 since we want to predict a single value in the output.\n",
        "    '''\n",
        "    model.summary()\n",
        "    model.compile(optimizer = \"adam\", loss = \"binary_crossentropy\")\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "dnn_model = KerasClassifier(build_fn=create_dnn, batch_size=32, verbose=1)\n",
        "\n",
        "callbacks = [\n",
        "    EarlyStopping(patience=10, verbose=1),\n",
        "    ReduceLROnPlateau(factor=0.1, patience=5, min_lr=0.00001, verbose=1)]\n",
        "\n",
        "history=dnn_model.fit(Feature_train_lstm, Label_train, batch_size = 32, epochs=15\n",
        "              ,validation_data=(Feature_test_lstm, Label_test), callbacks=callbacks)\n",
        "y_pred1=dnn_model.predict(Feature_test_lstm)\n",
        "\n",
        "y_pred1=dnn_model.predict(Feature_train_lstm)\n",
        "y_pred2=dnn_model.predict(Feature_test_lstm)\n",
        "print(y_pred2)\n",
        "print(y_pred1)\n",
        "\n",
        "\n",
        "\n",
        "plt.figure(figsize=(6,8))\n",
        "plt.title(\"Learning curve\")\n",
        "plt.plot(history.history[\"loss\"],'-b', label=\"loss\")\n",
        "plt.plot(history.history[\"val_loss\"],'-r', label=\"val_loss\")\n",
        "plt.plot( np.argmin(history.history[\"val_loss\"]), np.min(history.history[\"val_loss\"]), marker=\"x\", color=\"black\", label=\"best model\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"log_loss\")\n",
        "plt.legend();\n",
        "\n",
        "\n",
        "\n",
        "####################################### MLP #########################################\n",
        "\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "joinedtrain=np.concatenate((Feature_train,y_pred1), axis=1)\n",
        "joinedtest=np.concatenate((Feature_test,y_pred2), axis=1)\n",
        "\n",
        "print('MLP Classifier start from here:')\n",
        "#########################################################################################\n",
        "classifier=MLPClassifier(solver='lbfgs',alpha=0.0001, verbose=0, max_iter=200)\n",
        "classifier.fit(joinedtrain, Label_train);\n",
        "lstmmlp=classifier.predict(joinedtest)\n",
        "print('MLP Classifier end here:')\n",
        "\n",
        "\n",
        "print(\"Confusion matrix is: \", confusion_matrix(Label_test, lstmmlp))\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "f1score=f1_score(Label_test, lstmmlp, average='binary')\n",
        "print(\"F1score is: \", f1score)\n",
        "\n",
        "\n",
        "from sklearn.metrics import recall_score\n",
        "recall = recall_score(Label_test, lstmmlp)\n",
        "print('Recall score: %f' % recall)\n",
        "\n",
        "from sklearn.metrics import precision_score\n",
        "precision = precision_score(Label_test, lstmmlp)\n",
        "print('Precision: %f' % precision)\n",
        "\n",
        "\n",
        "# ROC AUC score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "auc = roc_auc_score(Label_test, lstmmlp)\n",
        "print('ROC AUC: %f' % auc)\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import plot_precision_recall_curve\n",
        "\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "MCC=matthews_corrcoef(Label_test, lstmmlp)\n",
        "print('MCC: %f' % MCC)\n",
        "\n",
        "\n",
        "\n",
        "# ROC AUC plot\n",
        "from sklearn.metrics import roc_curve\n",
        "fpr5_lr, tpr5_lr, _ = roc_curve(Label_test, lstmmlp)\n",
        "plt.figure(1)\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.plot(fpr5_lr, tpr5_lr, label='Proposed model')\n",
        "plt.xlabel('False positive rate')\n",
        "plt.ylabel('True positive rate')\n",
        "plt.title('ROC curve')\n",
        "plt.legend(loc='best')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# plot the precision-recall curves\n",
        "no_skill = len(Label_test[Label_test==1]) / len(Label_test)\n",
        "pyplot.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill')\n",
        "pyplot.plot(recall, precision, marker='.', label='Proposed')\n",
        "# axis labels\n",
        "pyplot.xlabel('Recall')\n",
        "pyplot.ylabel('Precision')\n",
        "# show the legend\n",
        "pyplot.legend()\n",
        "# show the plot\n",
        "pyplot.show()\n",
        "\n",
        "\n",
        "from sklearn.metrics import average_precision_score\n",
        "average_precision = average_precision_score(Label_test, lstmmlp)\n",
        "print('Average precision-recall score: {0:0.2f}'.format(\n",
        "      average_precision))\n",
        "\n",
        "\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import plot_precision_recall_curve\n",
        "import matplotlib.pyplot as plt\n",
        "disp = plot_precision_recall_curve(classifier, joinedtest, Label_test, name=\"Our proposed model\")\n",
        "disp.ax_.set_title('2-class Precision-Recall curve: '\n",
        "                   'AP={0:0.2f}'.format(average_precision))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "############################# Model 4 LSTM-MLP END ##############################\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.metrics import roc_curve\n",
        "fpr1_lr, tpr1_lr, _ = roc_curve(Label_test, predictions)\n",
        "fpr2_lr, tpr2_lr, _ = roc_curve(Label_test, wide_pred1)\n",
        "fpr3_lr, tpr3_lr, _ = roc_curve(Label_test, lstmcnnmlp)\n",
        "fpr4_lr, tpr4_lr, _ = roc_curve(Label_test, grumlp)\n",
        "fpr5_lr, tpr5_lr, _ = roc_curve(Label_test, lstmmlp)\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.plot(fpr1_lr, tpr1_lr,'b', label='Proposed model')\n",
        "plt.plot(fpr2_lr, tpr2_lr,'#8c564b', label='W&D-CNN')\n",
        "plt.plot(fpr3_lr, tpr3_lr,'r', label='CNN_LSTM-MLP')\n",
        "plt.plot(fpr4_lr, tpr4_lr,'#1f77b4', label='GRU-MLP')\n",
        "plt.plot(fpr5_lr, tpr5_lr,'#d62728', label='LSTM-MLP')\n",
        "plt.xlabel('False positive rate', fontsize=14)\n",
        "plt.ylabel('True positive rate', fontsize=14)\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------ Now Reading the data ------- Wait..\n",
            "   \n",
            "Normal usage: '28/2/2020 19:00'\n",
            "Normal usage: '14/2/2020 19:00'\n",
            "Normal usage: '11/2/2020 22:00'\n",
            "Normal usage: '2/2/2020 19:00'\n",
            "Sudden unusual consumption: '29/1/2020 22:00'\n",
            "Normal usage: '26/1/2020 23:00'\n",
            "Sudden unusual consumption: '13/1/2020 23:00'\n",
            "Possible illegal consumption: '5/1/2020 19:00'\n",
            "Normal usage: '19/12/2019 22:00'\n",
            "   \n",
            "------ Reading the data ------- Complete..\n",
            "------ fetching subset of data ------- wait..\n",
            "------ fetched subset of data ------- Complete..\n",
            "------ Imputation--Linear Interpolation method start------- wait..\n",
            "------ Imputation--Linear Interpolation method ------- Complete..\n",
            "------ Outlier detection and removal 3 sigma method start------- wait..\n",
            "Identified outliers: 23\n",
            "Non-outlier observations: 2977\n",
            "------ 3 sigma method ------- Complete..\n",
            "(2977, 1035)\n",
            "------ Proposed new sampling method start ------- wait..\n",
            "(2977, 1035)\n",
            "theftysamples are: 514\n",
            "honestysamples are: 2463\n",
            "their difference is: 1949\n",
            "their shapes are: (514, 1034) (2463, 1034)\n",
            "theft percentile values of 25 and 75: 0.0 14.04\n",
            "honest percentile values of 25 and 75: 0.0 5.36\n",
            "Total values of theft: 531476\n",
            "Total values of honest: 2546742\n",
            "percentage of data in theft q1: 0.0\n",
            "percentage of data in theft q2: 47.53836485560966\n",
            "percentage of data in theft q3: 24.99510796348283\n",
            "percentage of data in honest q1: 0.0\n",
            "percentage of data in honest q2: 235.99936779835778\n",
            "percentage of data in honest q3: 33.322294891961256\n",
            "number of % data in theft q1: 0.0\n",
            "number of % data in theft q2: 491.5466926070039\n",
            "number of % data in theft q3: 258.44941634241246\n",
            "number of % data in honest q1: 0.0\n",
            "number of % data in theft q2: 2440.2334630350197\n",
            "number of % data in theft q3: 344.55252918287937\n",
            "integer number of data in theft q1,2,3: 0 491 258\n",
            "integer number of data in honest q1,2,3: 0 2440 344\n",
            "number of 7.5% data from theft q1: 0.0\n",
            "number of 15% data from theft q2: 73.65\n",
            "number of 7.5% data from theft q3: 19.35\n",
            "this get from honest quantiles q1,2,3: 0 73 19\n",
            "number of 7.5% data from theft q1: 0\n",
            "number of 15% values subtract from theft data q2: 418\n",
            "number of 7.5% values subtract from theft data q3: 239\n",
            "this get from theft quantiles q1,2,3: 0 418 239\n",
            "honestrows 2463\n",
            "theftrow 514\n",
            "Balances 1949\n",
            "sample creation please wait\n",
            "sample creation please done\n",
            "new thefty samples are:  2463\n",
            "new honest samples are:  2463\n",
            "(4926, 1035)\n",
            "------ Proposed new sampling method  ------- completed..\n",
            "Data normalization---------------start\n",
            "Data normalization---------------Complete\n",
            "------ Pre-processing ...Done..\n",
            "------ Done..\n",
            "------ Splitting the Data ------- Wait..\n",
            "Training Features Shape: (3694, 1034)\n",
            "Training Labels Shape: (3694,)\n",
            "Testing Features Shape: (1232, 1034)\n",
            "Testing Labels Shape: (1232,)\n",
            "------ Done..\n",
            "################################\n",
            "(4926, 1034)\n",
            "(4926,)\n",
            "(4926, 128, 128, 1) (4926, 128, 128, 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:181: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ce1a1e6029d44b2d8694f6627702d14c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=4926.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "making 2d data and adding -0\n",
            "making 2d data and adding -1\n",
            "making 2d data and adding -2\n",
            "making 2d data and adding -3\n",
            "making 2d data and adding -4\n",
            "making 2d data and adding -5\n",
            "making 2d data and adding -6\n",
            "making 2d data and adding -7\n",
            "making 2d data and adding -8\n",
            "making 2d data and adding -9\n",
            "making 2d data and adding -10\n",
            "making 2d data and adding -11\n",
            "making 2d data and adding -12\n",
            "making 2d data and adding -13\n",
            "making 2d data and adding -14\n",
            "making 2d data and adding -15\n",
            "making 2d data and adding -16\n",
            "making 2d data and adding -17\n",
            "making 2d data and adding -18\n",
            "making 2d data and adding -19\n",
            "making 2d data and adding -20\n",
            "making 2d data and adding -21\n",
            "making 2d data and adding -22\n",
            "making 2d data and adding -23\n",
            "making 2d data and adding -24\n",
            "making 2d data and adding -25\n",
            "making 2d data and adding -26\n",
            "making 2d data and adding -27\n",
            "making 2d data and adding -28\n",
            "making 2d data and adding -29\n",
            "making 2d data and adding -30\n",
            "making 2d data and adding -31\n",
            "making 2d data and adding -32\n",
            "making 2d data and adding -33\n",
            "making 2d data and adding -34\n",
            "making 2d data and adding -35\n",
            "making 2d data and adding -36\n",
            "making 2d data and adding -37\n",
            "making 2d data and adding -38\n",
            "making 2d data and adding -39\n",
            "making 2d data and adding -40\n",
            "making 2d data and adding -41\n",
            "making 2d data and adding -42\n",
            "making 2d data and adding -43\n",
            "making 2d data and adding -44\n",
            "making 2d data and adding -45\n",
            "making 2d data and adding -46\n",
            "making 2d data and adding -47\n",
            "making 2d data and adding -48\n",
            "making 2d data and adding -49\n",
            "making 2d data and adding -50\n",
            "making 2d data and adding -51\n",
            "making 2d data and adding -52\n",
            "making 2d data and adding -53\n",
            "making 2d data and adding -54\n",
            "making 2d data and adding -55\n",
            "making 2d data and adding -56\n",
            "making 2d data and adding -57\n",
            "making 2d data and adding -58\n",
            "making 2d data and adding -59\n",
            "making 2d data and adding -60\n",
            "making 2d data and adding -61\n",
            "making 2d data and adding -62\n",
            "making 2d data and adding -63\n",
            "making 2d data and adding -64\n",
            "making 2d data and adding -65\n",
            "making 2d data and adding -66\n",
            "making 2d data and adding -67\n",
            "making 2d data and adding -68\n",
            "making 2d data and adding -69\n",
            "making 2d data and adding -70\n",
            "making 2d data and adding -71\n",
            "making 2d data and adding -72\n",
            "making 2d data and adding -73\n",
            "making 2d data and adding -74\n",
            "making 2d data and adding -75\n",
            "making 2d data and adding -76\n",
            "making 2d data and adding -77\n",
            "making 2d data and adding -78\n",
            "making 2d data and adding -79\n",
            "making 2d data and adding -80\n",
            "making 2d data and adding -81\n",
            "making 2d data and adding -82\n",
            "making 2d data and adding -83\n",
            "making 2d data and adding -84\n",
            "making 2d data and adding -85\n",
            "making 2d data and adding -86\n",
            "making 2d data and adding -87\n",
            "making 2d data and adding -88\n",
            "making 2d data and adding -89\n",
            "making 2d data and adding -90\n",
            "making 2d data and adding -91\n",
            "making 2d data and adding -92\n",
            "making 2d data and adding -93\n",
            "making 2d data and adding -94\n",
            "making 2d data and adding -95\n",
            "making 2d data and adding -96\n",
            "making 2d data and adding -97\n",
            "making 2d data and adding -98\n",
            "making 2d data and adding -99\n",
            "making 2d data and adding -100\n",
            "making 2d data and adding -101\n",
            "making 2d data and adding -102\n",
            "making 2d data and adding -103\n",
            "making 2d data and adding -104\n",
            "making 2d data and adding -105\n",
            "making 2d data and adding -106\n",
            "making 2d data and adding -107\n",
            "making 2d data and adding -108\n",
            "making 2d data and adding -109\n",
            "making 2d data and adding -110\n",
            "making 2d data and adding -111\n",
            "making 2d data and adding -112\n",
            "making 2d data and adding -113\n",
            "making 2d data and adding -114\n",
            "making 2d data and adding -115\n",
            "making 2d data and adding -116\n",
            "making 2d data and adding -117\n",
            "making 2d data and adding -118\n",
            "making 2d data and adding -119\n",
            "making 2d data and adding -120\n",
            "making 2d data and adding -121\n",
            "making 2d data and adding -122\n",
            "making 2d data and adding -123\n",
            "making 2d data and adding -124\n",
            "making 2d data and adding -125\n",
            "making 2d data and adding -126\n",
            "making 2d data and adding -127\n",
            "making 2d data and adding -128\n",
            "making 2d data and adding -129\n",
            "making 2d data and adding -130\n",
            "making 2d data and adding -131\n",
            "making 2d data and adding -132\n",
            "making 2d data and adding -133\n",
            "making 2d data and adding -134\n",
            "making 2d data and adding -135\n",
            "making 2d data and adding -136\n",
            "making 2d data and adding -137\n",
            "making 2d data and adding -138\n",
            "making 2d data and adding -139\n",
            "making 2d data and adding -140\n",
            "making 2d data and adding -141\n",
            "making 2d data and adding -142\n",
            "making 2d data and adding -143\n",
            "making 2d data and adding -144\n",
            "making 2d data and adding -145\n",
            "making 2d data and adding -146\n",
            "making 2d data and adding -147\n",
            "making 2d data and adding -148\n",
            "making 2d data and adding -149\n",
            "making 2d data and adding -150\n",
            "making 2d data and adding -151\n",
            "making 2d data and adding -152\n",
            "making 2d data and adding -153\n",
            "making 2d data and adding -154\n",
            "making 2d data and adding -155\n",
            "making 2d data and adding -156\n",
            "making 2d data and adding -157\n",
            "making 2d data and adding -158\n",
            "making 2d data and adding -159\n",
            "making 2d data and adding -160\n",
            "making 2d data and adding -161\n",
            "making 2d data and adding -162\n",
            "making 2d data and adding -163\n",
            "making 2d data and adding -164\n",
            "making 2d data and adding -165\n",
            "making 2d data and adding -166\n",
            "making 2d data and adding -167\n",
            "making 2d data and adding -168\n",
            "making 2d data and adding -169\n",
            "making 2d data and adding -170\n",
            "making 2d data and adding -171\n",
            "making 2d data and adding -172\n",
            "making 2d data and adding -173\n",
            "making 2d data and adding -174\n",
            "making 2d data and adding -175\n",
            "making 2d data and adding -176\n",
            "making 2d data and adding -177\n",
            "making 2d data and adding -178\n",
            "making 2d data and adding -179\n",
            "making 2d data and adding -180\n",
            "making 2d data and adding -181\n",
            "making 2d data and adding -182\n",
            "making 2d data and adding -183\n",
            "making 2d data and adding -184\n",
            "making 2d data and adding -185\n",
            "making 2d data and adding -186\n",
            "making 2d data and adding -187\n",
            "making 2d data and adding -188\n",
            "making 2d data and adding -189\n",
            "making 2d data and adding -190\n",
            "making 2d data and adding -191\n",
            "making 2d data and adding -192\n",
            "making 2d data and adding -193\n",
            "making 2d data and adding -194\n",
            "making 2d data and adding -195\n",
            "making 2d data and adding -196\n",
            "making 2d data and adding -197\n",
            "making 2d data and adding -198\n",
            "making 2d data and adding -199\n",
            "making 2d data and adding -200\n",
            "making 2d data and adding -201\n",
            "making 2d data and adding -202\n",
            "making 2d data and adding -203\n",
            "making 2d data and adding -204\n",
            "making 2d data and adding -205\n",
            "making 2d data and adding -206\n",
            "making 2d data and adding -207\n",
            "making 2d data and adding -208\n",
            "making 2d data and adding -209\n",
            "making 2d data and adding -210\n",
            "making 2d data and adding -211\n",
            "making 2d data and adding -212\n",
            "making 2d data and adding -213\n",
            "making 2d data and adding -214\n",
            "making 2d data and adding -215\n",
            "making 2d data and adding -216\n",
            "making 2d data and adding -217\n",
            "making 2d data and adding -218\n",
            "making 2d data and adding -219\n",
            "making 2d data and adding -220\n",
            "making 2d data and adding -221\n",
            "making 2d data and adding -222\n",
            "making 2d data and adding -223\n",
            "making 2d data and adding -224\n",
            "making 2d data and adding -225\n",
            "making 2d data and adding -226\n",
            "making 2d data and adding -227\n",
            "making 2d data and adding -228\n",
            "making 2d data and adding -229\n",
            "making 2d data and adding -230\n",
            "making 2d data and adding -231\n",
            "making 2d data and adding -232\n",
            "making 2d data and adding -233\n",
            "making 2d data and adding -234\n",
            "making 2d data and adding -235\n",
            "making 2d data and adding -236\n",
            "making 2d data and adding -237\n",
            "making 2d data and adding -238\n",
            "making 2d data and adding -239\n",
            "making 2d data and adding -240\n",
            "making 2d data and adding -241\n",
            "making 2d data and adding -242\n",
            "making 2d data and adding -243\n",
            "making 2d data and adding -244\n",
            "making 2d data and adding -245\n",
            "making 2d data and adding -246\n",
            "making 2d data and adding -247\n",
            "making 2d data and adding -248\n",
            "making 2d data and adding -249\n",
            "making 2d data and adding -250\n",
            "making 2d data and adding -251\n",
            "making 2d data and adding -252\n",
            "making 2d data and adding -253\n",
            "making 2d data and adding -254\n",
            "making 2d data and adding -255\n",
            "making 2d data and adding -256\n",
            "making 2d data and adding -257\n",
            "making 2d data and adding -258\n",
            "making 2d data and adding -259\n",
            "making 2d data and adding -260\n",
            "making 2d data and adding -261\n",
            "making 2d data and adding -262\n",
            "making 2d data and adding -263\n",
            "making 2d data and adding -264\n",
            "making 2d data and adding -265\n",
            "making 2d data and adding -266\n",
            "making 2d data and adding -267\n",
            "making 2d data and adding -268\n",
            "making 2d data and adding -269\n",
            "making 2d data and adding -270\n",
            "making 2d data and adding -271\n",
            "making 2d data and adding -272\n",
            "making 2d data and adding -273\n",
            "making 2d data and adding -274\n",
            "making 2d data and adding -275\n",
            "making 2d data and adding -276\n",
            "making 2d data and adding -277\n",
            "making 2d data and adding -278\n",
            "making 2d data and adding -279\n",
            "making 2d data and adding -280\n",
            "making 2d data and adding -281\n",
            "making 2d data and adding -282\n",
            "making 2d data and adding -283\n",
            "making 2d data and adding -284\n",
            "making 2d data and adding -285\n",
            "making 2d data and adding -286\n",
            "making 2d data and adding -287\n",
            "making 2d data and adding -288\n",
            "making 2d data and adding -289\n",
            "making 2d data and adding -290\n",
            "making 2d data and adding -291\n",
            "making 2d data and adding -292\n",
            "making 2d data and adding -293\n",
            "making 2d data and adding -294\n",
            "making 2d data and adding -295\n",
            "making 2d data and adding -296\n",
            "making 2d data and adding -297\n",
            "making 2d data and adding -298\n",
            "making 2d data and adding -299\n",
            "making 2d data and adding -300\n",
            "making 2d data and adding -301\n",
            "making 2d data and adding -302\n",
            "making 2d data and adding -303\n",
            "making 2d data and adding -304\n",
            "making 2d data and adding -305\n",
            "making 2d data and adding -306\n",
            "making 2d data and adding -307\n",
            "making 2d data and adding -308\n",
            "making 2d data and adding -309\n",
            "making 2d data and adding -310\n",
            "making 2d data and adding -311\n",
            "making 2d data and adding -312\n",
            "making 2d data and adding -313\n",
            "making 2d data and adding -314\n",
            "making 2d data and adding -315\n",
            "making 2d data and adding -316\n",
            "making 2d data and adding -317\n",
            "making 2d data and adding -318\n",
            "making 2d data and adding -319\n",
            "making 2d data and adding -320\n",
            "making 2d data and adding -321\n",
            "making 2d data and adding -322\n",
            "making 2d data and adding -323\n",
            "making 2d data and adding -324\n",
            "making 2d data and adding -325\n",
            "making 2d data and adding -326\n",
            "making 2d data and adding -327\n",
            "making 2d data and adding -328\n",
            "making 2d data and adding -329\n",
            "making 2d data and adding -330\n",
            "making 2d data and adding -331\n",
            "making 2d data and adding -332\n",
            "making 2d data and adding -333\n",
            "making 2d data and adding -334\n",
            "making 2d data and adding -335\n",
            "making 2d data and adding -336\n",
            "making 2d data and adding -337\n",
            "making 2d data and adding -338\n",
            "making 2d data and adding -339\n",
            "making 2d data and adding -340\n",
            "making 2d data and adding -341\n",
            "making 2d data and adding -342\n",
            "making 2d data and adding -343\n",
            "making 2d data and adding -344\n",
            "making 2d data and adding -345\n",
            "making 2d data and adding -346\n",
            "making 2d data and adding -347\n",
            "making 2d data and adding -348\n",
            "making 2d data and adding -349\n",
            "making 2d data and adding -350\n",
            "making 2d data and adding -351\n",
            "making 2d data and adding -352\n",
            "making 2d data and adding -353\n",
            "making 2d data and adding -354\n",
            "making 2d data and adding -355\n",
            "making 2d data and adding -356\n",
            "making 2d data and adding -357\n",
            "making 2d data and adding -358\n",
            "making 2d data and adding -359\n",
            "making 2d data and adding -360\n",
            "making 2d data and adding -361\n",
            "making 2d data and adding -362\n",
            "making 2d data and adding -363\n",
            "making 2d data and adding -364\n",
            "making 2d data and adding -365\n",
            "making 2d data and adding -366\n",
            "making 2d data and adding -367\n",
            "making 2d data and adding -368\n",
            "making 2d data and adding -369\n",
            "making 2d data and adding -370\n",
            "making 2d data and adding -371\n",
            "making 2d data and adding -372\n",
            "making 2d data and adding -373\n",
            "making 2d data and adding -374\n",
            "making 2d data and adding -375\n",
            "making 2d data and adding -376\n",
            "making 2d data and adding -377\n",
            "making 2d data and adding -378\n",
            "making 2d data and adding -379\n",
            "making 2d data and adding -380\n",
            "making 2d data and adding -381\n",
            "making 2d data and adding -382\n",
            "making 2d data and adding -383\n",
            "making 2d data and adding -384\n",
            "making 2d data and adding -385\n",
            "making 2d data and adding -386\n",
            "making 2d data and adding -387\n",
            "making 2d data and adding -388\n",
            "making 2d data and adding -389\n",
            "making 2d data and adding -390\n",
            "making 2d data and adding -391\n",
            "making 2d data and adding -392\n",
            "making 2d data and adding -393\n",
            "making 2d data and adding -394\n",
            "making 2d data and adding -395\n",
            "making 2d data and adding -396\n",
            "making 2d data and adding -397\n",
            "making 2d data and adding -398\n",
            "making 2d data and adding -399\n",
            "making 2d data and adding -400\n",
            "making 2d data and adding -401\n",
            "making 2d data and adding -402\n",
            "making 2d data and adding -403\n",
            "making 2d data and adding -404\n",
            "making 2d data and adding -405\n",
            "making 2d data and adding -406\n",
            "making 2d data and adding -407\n",
            "making 2d data and adding -408\n",
            "making 2d data and adding -409\n",
            "making 2d data and adding -410\n",
            "making 2d data and adding -411\n",
            "making 2d data and adding -412\n",
            "making 2d data and adding -413\n",
            "making 2d data and adding -414\n",
            "making 2d data and adding -415\n",
            "making 2d data and adding -416\n",
            "making 2d data and adding -417\n",
            "making 2d data and adding -418\n",
            "making 2d data and adding -419\n",
            "making 2d data and adding -420\n",
            "making 2d data and adding -421\n",
            "making 2d data and adding -422\n",
            "making 2d data and adding -423\n",
            "making 2d data and adding -424\n",
            "making 2d data and adding -425\n",
            "making 2d data and adding -426\n",
            "making 2d data and adding -427\n",
            "making 2d data and adding -428\n",
            "making 2d data and adding -429\n",
            "making 2d data and adding -430\n",
            "making 2d data and adding -431\n",
            "making 2d data and adding -432\n",
            "making 2d data and adding -433\n",
            "making 2d data and adding -434\n",
            "making 2d data and adding -435\n",
            "making 2d data and adding -436\n",
            "making 2d data and adding -437\n",
            "making 2d data and adding -438\n",
            "making 2d data and adding -439\n",
            "making 2d data and adding -440\n",
            "making 2d data and adding -441\n",
            "making 2d data and adding -442\n",
            "making 2d data and adding -443\n",
            "making 2d data and adding -444\n",
            "making 2d data and adding -445\n",
            "making 2d data and adding -446\n",
            "making 2d data and adding -447\n",
            "making 2d data and adding -448\n",
            "making 2d data and adding -449\n",
            "making 2d data and adding -450\n",
            "making 2d data and adding -451\n",
            "making 2d data and adding -452\n",
            "making 2d data and adding -453\n",
            "making 2d data and adding -454\n",
            "making 2d data and adding -455\n",
            "making 2d data and adding -456\n",
            "making 2d data and adding -457\n",
            "making 2d data and adding -458\n",
            "making 2d data and adding -459\n",
            "making 2d data and adding -460\n",
            "making 2d data and adding -461\n",
            "making 2d data and adding -462\n",
            "making 2d data and adding -463\n",
            "making 2d data and adding -464\n",
            "making 2d data and adding -465\n",
            "making 2d data and adding -466\n",
            "making 2d data and adding -467\n",
            "making 2d data and adding -468\n",
            "making 2d data and adding -469\n",
            "making 2d data and adding -470\n",
            "making 2d data and adding -471\n",
            "making 2d data and adding -472\n",
            "making 2d data and adding -473\n",
            "making 2d data and adding -474\n",
            "making 2d data and adding -475\n",
            "making 2d data and adding -476\n",
            "making 2d data and adding -477\n",
            "making 2d data and adding -478\n",
            "making 2d data and adding -479\n",
            "making 2d data and adding -480\n",
            "making 2d data and adding -481\n",
            "making 2d data and adding -482\n",
            "making 2d data and adding -483\n",
            "making 2d data and adding -484\n",
            "making 2d data and adding -485\n",
            "making 2d data and adding -486\n",
            "making 2d data and adding -487\n",
            "making 2d data and adding -488\n",
            "making 2d data and adding -489\n",
            "making 2d data and adding -490\n",
            "making 2d data and adding -491\n",
            "making 2d data and adding -492\n",
            "making 2d data and adding -493\n",
            "making 2d data and adding -494\n",
            "making 2d data and adding -495\n",
            "making 2d data and adding -496\n",
            "making 2d data and adding -497\n",
            "making 2d data and adding -498\n",
            "making 2d data and adding -499\n",
            "making 2d data and adding -500\n",
            "making 2d data and adding -501\n",
            "making 2d data and adding -502\n",
            "making 2d data and adding -503\n",
            "making 2d data and adding -504\n",
            "making 2d data and adding -505\n",
            "making 2d data and adding -506\n",
            "making 2d data and adding -507\n",
            "making 2d data and adding -508\n",
            "making 2d data and adding -509\n",
            "making 2d data and adding -510\n",
            "making 2d data and adding -511\n",
            "making 2d data and adding -512\n",
            "making 2d data and adding -513\n",
            "making 2d data and adding -514\n",
            "making 2d data and adding -515\n",
            "making 2d data and adding -516\n",
            "making 2d data and adding -517\n",
            "making 2d data and adding -518\n",
            "making 2d data and adding -519\n",
            "making 2d data and adding -520\n",
            "making 2d data and adding -521\n",
            "making 2d data and adding -522\n",
            "making 2d data and adding -523\n",
            "making 2d data and adding -524\n",
            "making 2d data and adding -525\n",
            "making 2d data and adding -526\n",
            "making 2d data and adding -527\n",
            "making 2d data and adding -528\n",
            "making 2d data and adding -529\n",
            "making 2d data and adding -530\n",
            "making 2d data and adding -531\n",
            "making 2d data and adding -532\n",
            "making 2d data and adding -533\n",
            "making 2d data and adding -534\n",
            "making 2d data and adding -535\n",
            "making 2d data and adding -536\n",
            "making 2d data and adding -537\n",
            "making 2d data and adding -538\n",
            "making 2d data and adding -539\n",
            "making 2d data and adding -540\n",
            "making 2d data and adding -541\n",
            "making 2d data and adding -542\n",
            "making 2d data and adding -543\n",
            "making 2d data and adding -544\n",
            "making 2d data and adding -545\n",
            "making 2d data and adding -546\n",
            "making 2d data and adding -547\n",
            "making 2d data and adding -548\n",
            "making 2d data and adding -549\n",
            "making 2d data and adding -550\n",
            "making 2d data and adding -551\n",
            "making 2d data and adding -552\n",
            "making 2d data and adding -553\n",
            "making 2d data and adding -554\n",
            "making 2d data and adding -555\n",
            "making 2d data and adding -556\n",
            "making 2d data and adding -557\n",
            "making 2d data and adding -558\n",
            "making 2d data and adding -559\n",
            "making 2d data and adding -560\n",
            "making 2d data and adding -561\n",
            "making 2d data and adding -562\n",
            "making 2d data and adding -563\n",
            "making 2d data and adding -564\n",
            "making 2d data and adding -565\n",
            "making 2d data and adding -566\n",
            "making 2d data and adding -567\n",
            "making 2d data and adding -568\n",
            "making 2d data and adding -569\n",
            "making 2d data and adding -570\n",
            "making 2d data and adding -571\n",
            "making 2d data and adding -572\n",
            "making 2d data and adding -573\n",
            "making 2d data and adding -574\n",
            "making 2d data and adding -575\n",
            "making 2d data and adding -576\n",
            "making 2d data and adding -577\n",
            "making 2d data and adding -578\n",
            "making 2d data and adding -579\n",
            "making 2d data and adding -580\n",
            "making 2d data and adding -581\n",
            "making 2d data and adding -582\n",
            "making 2d data and adding -583\n",
            "making 2d data and adding -584\n",
            "making 2d data and adding -585\n",
            "making 2d data and adding -586\n",
            "making 2d data and adding -587\n",
            "making 2d data and adding -588\n",
            "making 2d data and adding -589\n",
            "making 2d data and adding -590\n",
            "making 2d data and adding -591\n",
            "making 2d data and adding -592\n",
            "making 2d data and adding -593\n",
            "making 2d data and adding -594\n",
            "making 2d data and adding -595\n",
            "making 2d data and adding -596\n",
            "making 2d data and adding -597\n",
            "making 2d data and adding -598\n",
            "making 2d data and adding -599\n",
            "making 2d data and adding -600\n",
            "making 2d data and adding -601\n",
            "making 2d data and adding -602\n",
            "making 2d data and adding -603\n",
            "making 2d data and adding -604\n",
            "making 2d data and adding -605\n",
            "making 2d data and adding -606\n",
            "making 2d data and adding -607\n",
            "making 2d data and adding -608\n",
            "making 2d data and adding -609\n",
            "making 2d data and adding -610\n",
            "making 2d data and adding -611\n",
            "making 2d data and adding -612\n",
            "making 2d data and adding -613\n",
            "making 2d data and adding -614\n",
            "making 2d data and adding -615\n",
            "making 2d data and adding -616\n",
            "making 2d data and adding -617\n",
            "making 2d data and adding -618\n",
            "making 2d data and adding -619\n",
            "making 2d data and adding -620\n",
            "making 2d data and adding -621\n",
            "making 2d data and adding -622\n",
            "making 2d data and adding -623\n",
            "making 2d data and adding -624\n",
            "making 2d data and adding -625\n",
            "making 2d data and adding -626\n",
            "making 2d data and adding -627\n",
            "making 2d data and adding -628\n",
            "making 2d data and adding -629\n",
            "making 2d data and adding -630\n",
            "making 2d data and adding -631\n",
            "making 2d data and adding -632\n",
            "making 2d data and adding -633\n",
            "making 2d data and adding -634\n",
            "making 2d data and adding -635\n",
            "making 2d data and adding -636\n",
            "making 2d data and adding -637\n",
            "making 2d data and adding -638\n",
            "making 2d data and adding -639\n",
            "making 2d data and adding -640\n",
            "making 2d data and adding -641\n",
            "making 2d data and adding -642\n",
            "making 2d data and adding -643\n",
            "making 2d data and adding -644\n",
            "making 2d data and adding -645\n",
            "making 2d data and adding -646\n",
            "making 2d data and adding -647\n",
            "making 2d data and adding -648\n",
            "making 2d data and adding -649\n",
            "making 2d data and adding -650\n",
            "making 2d data and adding -651\n",
            "making 2d data and adding -652\n",
            "making 2d data and adding -653\n",
            "making 2d data and adding -654\n",
            "making 2d data and adding -655\n",
            "making 2d data and adding -656\n",
            "making 2d data and adding -657\n",
            "making 2d data and adding -658\n",
            "making 2d data and adding -659\n",
            "making 2d data and adding -660\n",
            "making 2d data and adding -661\n",
            "making 2d data and adding -662\n",
            "making 2d data and adding -663\n",
            "making 2d data and adding -664\n",
            "making 2d data and adding -665\n",
            "making 2d data and adding -666\n",
            "making 2d data and adding -667\n",
            "making 2d data and adding -668\n",
            "making 2d data and adding -669\n",
            "making 2d data and adding -670\n",
            "making 2d data and adding -671\n",
            "making 2d data and adding -672\n",
            "making 2d data and adding -673\n",
            "making 2d data and adding -674\n",
            "making 2d data and adding -675\n",
            "making 2d data and adding -676\n",
            "making 2d data and adding -677\n",
            "making 2d data and adding -678\n",
            "making 2d data and adding -679\n",
            "making 2d data and adding -680\n",
            "making 2d data and adding -681\n",
            "making 2d data and adding -682\n",
            "making 2d data and adding -683\n",
            "making 2d data and adding -684\n",
            "making 2d data and adding -685\n",
            "making 2d data and adding -686\n",
            "making 2d data and adding -687\n",
            "making 2d data and adding -688\n",
            "making 2d data and adding -689\n",
            "making 2d data and adding -690\n",
            "making 2d data and adding -691\n",
            "making 2d data and adding -692\n",
            "making 2d data and adding -693\n",
            "making 2d data and adding -694\n",
            "making 2d data and adding -695\n",
            "making 2d data and adding -696\n",
            "making 2d data and adding -697\n",
            "making 2d data and adding -698\n",
            "making 2d data and adding -699\n",
            "making 2d data and adding -700\n",
            "making 2d data and adding -701\n",
            "making 2d data and adding -702\n",
            "making 2d data and adding -703\n",
            "making 2d data and adding -704\n",
            "making 2d data and adding -705\n",
            "making 2d data and adding -706\n",
            "making 2d data and adding -707\n",
            "making 2d data and adding -708\n",
            "making 2d data and adding -709\n",
            "making 2d data and adding -710\n",
            "making 2d data and adding -711\n",
            "making 2d data and adding -712\n",
            "making 2d data and adding -713\n",
            "making 2d data and adding -714\n",
            "making 2d data and adding -715\n",
            "making 2d data and adding -716\n",
            "making 2d data and adding -717\n",
            "making 2d data and adding -718\n",
            "making 2d data and adding -719\n",
            "making 2d data and adding -720\n",
            "making 2d data and adding -721\n",
            "making 2d data and adding -722\n",
            "making 2d data and adding -723\n",
            "making 2d data and adding -724\n",
            "making 2d data and adding -725\n",
            "making 2d data and adding -726\n",
            "making 2d data and adding -727\n",
            "making 2d data and adding -728\n",
            "making 2d data and adding -729\n",
            "making 2d data and adding -730\n",
            "making 2d data and adding -731\n",
            "making 2d data and adding -732\n",
            "making 2d data and adding -733\n",
            "making 2d data and adding -734\n",
            "making 2d data and adding -735\n",
            "making 2d data and adding -736\n",
            "making 2d data and adding -737\n",
            "making 2d data and adding -738\n",
            "making 2d data and adding -739\n",
            "making 2d data and adding -740\n",
            "making 2d data and adding -741\n",
            "making 2d data and adding -742\n",
            "making 2d data and adding -743\n",
            "making 2d data and adding -744\n",
            "making 2d data and adding -745\n",
            "making 2d data and adding -746\n",
            "making 2d data and adding -747\n",
            "making 2d data and adding -748\n",
            "making 2d data and adding -749\n",
            "making 2d data and adding -750\n",
            "making 2d data and adding -751\n",
            "making 2d data and adding -752\n",
            "making 2d data and adding -753\n",
            "making 2d data and adding -754\n",
            "making 2d data and adding -755\n",
            "making 2d data and adding -756\n",
            "making 2d data and adding -757\n",
            "making 2d data and adding -758\n",
            "making 2d data and adding -759\n",
            "making 2d data and adding -760\n",
            "making 2d data and adding -761\n",
            "making 2d data and adding -762\n",
            "making 2d data and adding -763\n",
            "making 2d data and adding -764\n",
            "making 2d data and adding -765\n",
            "making 2d data and adding -766\n",
            "making 2d data and adding -767\n",
            "making 2d data and adding -768\n",
            "making 2d data and adding -769\n",
            "making 2d data and adding -770\n",
            "making 2d data and adding -771\n",
            "making 2d data and adding -772\n",
            "making 2d data and adding -773\n",
            "making 2d data and adding -774\n",
            "making 2d data and adding -775\n",
            "making 2d data and adding -776\n",
            "making 2d data and adding -777\n",
            "making 2d data and adding -778\n",
            "making 2d data and adding -779\n",
            "making 2d data and adding -780\n",
            "making 2d data and adding -781\n",
            "making 2d data and adding -782\n",
            "making 2d data and adding -783\n",
            "making 2d data and adding -784\n",
            "making 2d data and adding -785\n",
            "making 2d data and adding -786\n",
            "making 2d data and adding -787\n",
            "making 2d data and adding -788\n",
            "making 2d data and adding -789\n",
            "making 2d data and adding -790\n",
            "making 2d data and adding -791\n",
            "making 2d data and adding -792\n",
            "making 2d data and adding -793\n",
            "making 2d data and adding -794\n",
            "making 2d data and adding -795\n",
            "making 2d data and adding -796\n",
            "making 2d data and adding -797\n",
            "making 2d data and adding -798\n",
            "making 2d data and adding -799\n",
            "making 2d data and adding -800\n",
            "making 2d data and adding -801\n",
            "making 2d data and adding -802\n",
            "making 2d data and adding -803\n",
            "making 2d data and adding -804\n",
            "making 2d data and adding -805\n",
            "making 2d data and adding -806\n",
            "making 2d data and adding -807\n",
            "making 2d data and adding -808\n",
            "making 2d data and adding -809\n",
            "making 2d data and adding -810\n",
            "making 2d data and adding -811\n",
            "making 2d data and adding -812\n",
            "making 2d data and adding -813\n",
            "making 2d data and adding -814\n",
            "making 2d data and adding -815\n",
            "making 2d data and adding -816\n",
            "making 2d data and adding -817\n",
            "making 2d data and adding -818\n",
            "making 2d data and adding -819\n",
            "making 2d data and adding -820\n",
            "making 2d data and adding -821\n",
            "making 2d data and adding -822\n",
            "making 2d data and adding -823\n",
            "making 2d data and adding -824\n",
            "making 2d data and adding -825\n",
            "making 2d data and adding -826\n",
            "making 2d data and adding -827\n",
            "making 2d data and adding -828\n",
            "making 2d data and adding -829\n",
            "making 2d data and adding -830\n",
            "making 2d data and adding -831\n",
            "making 2d data and adding -832\n",
            "making 2d data and adding -833\n",
            "making 2d data and adding -834\n",
            "making 2d data and adding -835\n",
            "making 2d data and adding -836\n",
            "making 2d data and adding -837\n",
            "making 2d data and adding -838\n",
            "making 2d data and adding -839\n",
            "making 2d data and adding -840\n",
            "making 2d data and adding -841\n",
            "making 2d data and adding -842\n",
            "making 2d data and adding -843\n",
            "making 2d data and adding -844\n",
            "making 2d data and adding -845\n",
            "making 2d data and adding -846\n",
            "making 2d data and adding -847\n",
            "making 2d data and adding -848\n",
            "making 2d data and adding -849\n",
            "making 2d data and adding -850\n",
            "making 2d data and adding -851\n",
            "making 2d data and adding -852\n",
            "making 2d data and adding -853\n",
            "making 2d data and adding -854\n",
            "making 2d data and adding -855\n",
            "making 2d data and adding -856\n",
            "making 2d data and adding -857\n",
            "making 2d data and adding -858\n",
            "making 2d data and adding -859\n",
            "making 2d data and adding -860\n",
            "making 2d data and adding -861\n",
            "making 2d data and adding -862\n",
            "making 2d data and adding -863\n",
            "making 2d data and adding -864\n",
            "making 2d data and adding -865\n",
            "making 2d data and adding -866\n",
            "making 2d data and adding -867\n",
            "making 2d data and adding -868\n",
            "making 2d data and adding -869\n",
            "making 2d data and adding -870\n",
            "making 2d data and adding -871\n",
            "making 2d data and adding -872\n",
            "making 2d data and adding -873\n",
            "making 2d data and adding -874\n",
            "making 2d data and adding -875\n",
            "making 2d data and adding -876\n",
            "making 2d data and adding -877\n",
            "making 2d data and adding -878\n",
            "making 2d data and adding -879\n",
            "making 2d data and adding -880\n",
            "making 2d data and adding -881\n",
            "making 2d data and adding -882\n",
            "making 2d data and adding -883\n",
            "making 2d data and adding -884\n",
            "making 2d data and adding -885\n",
            "making 2d data and adding -886\n",
            "making 2d data and adding -887\n",
            "making 2d data and adding -888\n",
            "making 2d data and adding -889\n",
            "making 2d data and adding -890\n",
            "making 2d data and adding -891\n",
            "making 2d data and adding -892\n",
            "making 2d data and adding -893\n",
            "making 2d data and adding -894\n",
            "making 2d data and adding -895\n",
            "making 2d data and adding -896\n",
            "making 2d data and adding -897\n",
            "making 2d data and adding -898\n",
            "making 2d data and adding -899\n",
            "making 2d data and adding -900\n",
            "making 2d data and adding -901\n",
            "making 2d data and adding -902\n",
            "making 2d data and adding -903\n",
            "making 2d data and adding -904\n",
            "making 2d data and adding -905\n",
            "making 2d data and adding -906\n",
            "making 2d data and adding -907\n",
            "making 2d data and adding -908\n",
            "making 2d data and adding -909\n",
            "making 2d data and adding -910\n",
            "making 2d data and adding -911\n",
            "making 2d data and adding -912\n",
            "making 2d data and adding -913\n",
            "making 2d data and adding -914\n",
            "making 2d data and adding -915\n",
            "making 2d data and adding -916\n",
            "making 2d data and adding -917\n",
            "making 2d data and adding -918\n",
            "making 2d data and adding -919\n",
            "making 2d data and adding -920\n",
            "making 2d data and adding -921\n",
            "making 2d data and adding -922\n",
            "making 2d data and adding -923\n",
            "making 2d data and adding -924\n",
            "making 2d data and adding -925\n",
            "making 2d data and adding -926\n",
            "making 2d data and adding -927\n",
            "making 2d data and adding -928\n",
            "making 2d data and adding -929\n",
            "making 2d data and adding -930\n",
            "making 2d data and adding -931\n",
            "making 2d data and adding -932\n",
            "making 2d data and adding -933\n",
            "making 2d data and adding -934\n",
            "making 2d data and adding -935\n",
            "making 2d data and adding -936\n",
            "making 2d data and adding -937\n",
            "making 2d data and adding -938\n",
            "making 2d data and adding -939\n",
            "making 2d data and adding -940\n",
            "making 2d data and adding -941\n",
            "making 2d data and adding -942\n",
            "making 2d data and adding -943\n",
            "making 2d data and adding -944\n",
            "making 2d data and adding -945\n",
            "making 2d data and adding -946\n",
            "making 2d data and adding -947\n",
            "making 2d data and adding -948\n",
            "making 2d data and adding -949\n",
            "making 2d data and adding -950\n",
            "making 2d data and adding -951\n",
            "making 2d data and adding -952\n",
            "making 2d data and adding -953\n",
            "making 2d data and adding -954\n",
            "making 2d data and adding -955\n",
            "making 2d data and adding -956\n",
            "making 2d data and adding -957\n",
            "making 2d data and adding -958\n",
            "making 2d data and adding -959\n",
            "making 2d data and adding -960\n",
            "making 2d data and adding -961\n",
            "making 2d data and adding -962\n",
            "making 2d data and adding -963\n",
            "making 2d data and adding -964\n",
            "making 2d data and adding -965\n",
            "making 2d data and adding -966\n",
            "making 2d data and adding -967\n",
            "making 2d data and adding -968\n",
            "making 2d data and adding -969\n",
            "making 2d data and adding -970\n",
            "making 2d data and adding -971\n",
            "making 2d data and adding -972\n",
            "making 2d data and adding -973\n",
            "making 2d data and adding -974\n",
            "making 2d data and adding -975\n",
            "making 2d data and adding -976\n",
            "making 2d data and adding -977\n",
            "making 2d data and adding -978\n",
            "making 2d data and adding -979\n",
            "making 2d data and adding -980\n",
            "making 2d data and adding -981\n",
            "making 2d data and adding -982\n",
            "making 2d data and adding -983\n",
            "making 2d data and adding -984\n",
            "making 2d data and adding -985\n",
            "making 2d data and adding -986\n",
            "making 2d data and adding -987\n",
            "making 2d data and adding -988\n",
            "making 2d data and adding -989\n",
            "making 2d data and adding -990\n",
            "making 2d data and adding -991\n",
            "making 2d data and adding -992\n",
            "making 2d data and adding -993\n",
            "making 2d data and adding -994\n",
            "making 2d data and adding -995\n",
            "making 2d data and adding -996\n",
            "making 2d data and adding -997\n",
            "making 2d data and adding -998\n",
            "making 2d data and adding -999\n",
            "making 2d data and adding -1000\n",
            "making 2d data and adding -1001\n",
            "making 2d data and adding -1002\n",
            "making 2d data and adding -1003\n",
            "making 2d data and adding -1004\n",
            "making 2d data and adding -1005\n",
            "making 2d data and adding -1006\n",
            "making 2d data and adding -1007\n",
            "making 2d data and adding -1008\n",
            "making 2d data and adding -1009\n",
            "making 2d data and adding -1010\n",
            "making 2d data and adding -1011\n",
            "making 2d data and adding -1012\n",
            "making 2d data and adding -1013\n",
            "making 2d data and adding -1014\n",
            "making 2d data and adding -1015\n",
            "making 2d data and adding -1016\n",
            "making 2d data and adding -1017\n",
            "making 2d data and adding -1018\n",
            "making 2d data and adding -1019\n",
            "making 2d data and adding -1020\n",
            "making 2d data and adding -1021\n",
            "making 2d data and adding -1022\n",
            "making 2d data and adding -1023\n",
            "making 2d data and adding -1024\n",
            "making 2d data and adding -1025\n",
            "making 2d data and adding -1026\n",
            "making 2d data and adding -1027\n",
            "making 2d data and adding -1028\n",
            "making 2d data and adding -1029\n",
            "making 2d data and adding -1030\n",
            "making 2d data and adding -1031\n",
            "making 2d data and adding -1032\n",
            "making 2d data and adding -1033\n",
            "\n",
            "[[[[0.0050261 ]\n",
            "   [0.00767683]\n",
            "   [0.00497559]\n",
            "   ...\n",
            "   [0.01920697]\n",
            "   [0.02132697]\n",
            "   [0.01768372]]\n",
            "\n",
            "  [[0.005495  ]\n",
            "   [0.00696596]\n",
            "   [0.00532097]\n",
            "   ...\n",
            "   [0.02219002]\n",
            "   [0.02243887]\n",
            "   [0.01896234]]\n",
            "\n",
            "  [[0.00688996]\n",
            "   [0.00714865]\n",
            "   [0.00428507]\n",
            "   ...\n",
            "   [0.02463611]\n",
            "   [0.02495697]\n",
            "   [0.02175895]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[0.00369722]\n",
            "   [0.00427138]\n",
            "   [0.00417346]\n",
            "   ...\n",
            "   [0.00403303]\n",
            "   [0.00427139]\n",
            "   [0.0037137 ]]\n",
            "\n",
            "  [[0.00358884]\n",
            "   [0.00409153]\n",
            "   [0.00413921]\n",
            "   ...\n",
            "   [0.00396727]\n",
            "   [0.00393827]\n",
            "   [0.00339491]]\n",
            "\n",
            "  [[0.00299641]\n",
            "   [0.00354403]\n",
            "   [0.00358291]\n",
            "   ...\n",
            "   [0.00342857]\n",
            "   [0.00330478]\n",
            "   [0.00282934]]]\n",
            "\n",
            "\n",
            " [[[0.0050261 ]\n",
            "   [0.00767683]\n",
            "   [0.00497559]\n",
            "   ...\n",
            "   [0.01920697]\n",
            "   [0.02132697]\n",
            "   [0.01768372]]\n",
            "\n",
            "  [[0.005495  ]\n",
            "   [0.00696596]\n",
            "   [0.00532097]\n",
            "   ...\n",
            "   [0.02219002]\n",
            "   [0.02243887]\n",
            "   [0.01896234]]\n",
            "\n",
            "  [[0.00688996]\n",
            "   [0.00714865]\n",
            "   [0.00428507]\n",
            "   ...\n",
            "   [0.02463611]\n",
            "   [0.02495697]\n",
            "   [0.02175895]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[0.00369722]\n",
            "   [0.00427138]\n",
            "   [0.00417346]\n",
            "   ...\n",
            "   [0.00403303]\n",
            "   [0.00427139]\n",
            "   [0.0037137 ]]\n",
            "\n",
            "  [[0.00358884]\n",
            "   [0.00409153]\n",
            "   [0.00413921]\n",
            "   ...\n",
            "   [0.00396727]\n",
            "   [0.00393827]\n",
            "   [0.00339491]]\n",
            "\n",
            "  [[0.00299641]\n",
            "   [0.00354403]\n",
            "   [0.00358291]\n",
            "   ...\n",
            "   [0.00342857]\n",
            "   [0.00330478]\n",
            "   [0.00282934]]]\n",
            "\n",
            "\n",
            " [[[0.0050261 ]\n",
            "   [0.00767683]\n",
            "   [0.00497559]\n",
            "   ...\n",
            "   [0.01920697]\n",
            "   [0.02132697]\n",
            "   [0.01768372]]\n",
            "\n",
            "  [[0.005495  ]\n",
            "   [0.00696596]\n",
            "   [0.00532097]\n",
            "   ...\n",
            "   [0.02219002]\n",
            "   [0.02243887]\n",
            "   [0.01896234]]\n",
            "\n",
            "  [[0.00688996]\n",
            "   [0.00714865]\n",
            "   [0.00428507]\n",
            "   ...\n",
            "   [0.02463611]\n",
            "   [0.02495697]\n",
            "   [0.02175895]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[0.00369722]\n",
            "   [0.00427138]\n",
            "   [0.00417346]\n",
            "   ...\n",
            "   [0.00403303]\n",
            "   [0.00427139]\n",
            "   [0.0037137 ]]\n",
            "\n",
            "  [[0.00358884]\n",
            "   [0.00409153]\n",
            "   [0.00413921]\n",
            "   ...\n",
            "   [0.00396727]\n",
            "   [0.00393827]\n",
            "   [0.00339491]]\n",
            "\n",
            "  [[0.00299641]\n",
            "   [0.00354403]\n",
            "   [0.00358291]\n",
            "   ...\n",
            "   [0.00342857]\n",
            "   [0.00330478]\n",
            "   [0.00282934]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   ...\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]]\n",
            "\n",
            "  [[0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   ...\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]]\n",
            "\n",
            "  [[0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   ...\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   ...\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]]\n",
            "\n",
            "  [[0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   ...\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]]\n",
            "\n",
            "  [[0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   ...\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]]]\n",
            "\n",
            "\n",
            " [[[0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   ...\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]]\n",
            "\n",
            "  [[0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   ...\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]]\n",
            "\n",
            "  [[0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   ...\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   ...\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]]\n",
            "\n",
            "  [[0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   ...\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]]\n",
            "\n",
            "  [[0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   ...\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]]]\n",
            "\n",
            "\n",
            " [[[0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   ...\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]]\n",
            "\n",
            "  [[0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   ...\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]]\n",
            "\n",
            "  [[0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   ...\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   ...\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]]\n",
            "\n",
            "  [[0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   ...\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]]\n",
            "\n",
            "  [[0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   ...\n",
            "   [0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]]]]\n",
            "Training Features Shape: (3694, 128, 128, 1)\n",
            "Training Labels Shape: (3694, 128, 128, 1)\n",
            "Testing Features Shape: (1232, 128, 128, 1)\n",
            "Testing Labels Shape: (1232, 128, 128, 1)\n",
            "------ Done..\n",
            "3694\n",
            "1034\n",
            "(3694, 1034, 1) (3694,)\n",
            "(3694, 1034, 1) (3694, 1)\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru (GRU)                    (None, 1034, 30)          2970      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu (LeakyReLU)      (None, 1034, 30)          0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 1034, 30)          0         \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 1034, 30)          120       \n",
            "_________________________________________________________________\n",
            "gru_1 (GRU)                  (None, 10)                1260      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)    (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 11        \n",
            "=================================================================\n",
            "Total params: 4,361\n",
            "Trainable params: 4,301\n",
            "Non-trainable params: 60\n",
            "_________________________________________________________________\n",
            "Epoch 1/15\n",
            "116/116 [==============================] - 90s 780ms/step - loss: 0.7530 - val_loss: 0.6987\n",
            "Epoch 2/15\n",
            "116/116 [==============================] - 95s 819ms/step - loss: 0.6606 - val_loss: 0.6912\n",
            "Epoch 3/15\n",
            "116/116 [==============================] - 90s 780ms/step - loss: 0.6496 - val_loss: 0.6790\n",
            "Epoch 4/15\n",
            "116/116 [==============================] - 90s 780ms/step - loss: 0.6290 - val_loss: 0.6604\n",
            "Epoch 5/15\n",
            "116/116 [==============================] - 91s 786ms/step - loss: 0.6321 - val_loss: 1.0160\n",
            "Epoch 6/15\n",
            "116/116 [==============================] - 91s 789ms/step - loss: 0.6504 - val_loss: 0.7174\n",
            "Epoch 7/15\n",
            " 87/116 [=====================>........] - ETA: 21s - loss: 0.7091"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHTlFeJGngDF"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}